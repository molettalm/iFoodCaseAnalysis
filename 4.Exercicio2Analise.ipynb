{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# ==============================================================================\n","# DISCLAIMER IMPORTANTE PARA AMBIENTES GOOGLE COLAB\n","# ==============================================================================\n","# Como o Google Colab fornece um ambiente de execução temporário, ele não vem\n","# com Java ou Spark pré-instalados. Portanto, este bloco de código DEVE SER\n","# EXECUTADO TODA VEZ que você iniciar ou reiniciar uma sessão (runtime) no Colab\n","# para instalar e configurar todas as dependências necessárias para o PySpark.\n","# ==============================================================================\n","\n","\n","# --- Bloco de Instalação e Configuração do Ambiente Spark ---\n","\n","# 1. Instalação do Java\n","# O Apache Spark é executado sobre a Java Virtual Machine (JVM), então o Java é um pré-requisito obrigatório.\n","# !apt-get update -qq: Atualiza a lista de pacotes do sistema operacional (baseado em Debian/Ubuntu). O '-qq' torna a saída mais silenciosa.\n","!apt-get update -qq\n","# !apt-get install: Instala o OpenJDK 11 (uma versão de código aberto do Java) sem interação do usuário (-y).\n","!apt-get install -y openjdk-11-jdk-headless\n","\n","# 2. Download do Spark\n","# Baixa os binários pré-compilados do Apache Spark a partir do site oficial de arquivamento da Apache.\n","# !wget -q: Baixa o arquivo da URL especificada. O '-q' (quiet) minimiza as mensagens de log durante o download.\n","!wget -q https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n","\n","# 3. Extração do Spark\n","# Descompacta o arquivo .tgz que foi baixado no passo anterior.\n","# !tar -xvzf:\n","#   x: eXtract (extrair)\n","#   v: verbose (mostra os arquivos sendo extraídos)\n","#   z: gZip (indica que o arquivo está compactado com gzip)\n","#   f: file (especifica o nome do arquivo a ser descompactado)\n","!tar -xvf spark-3.5.1-bin-hadoop3.tgz\n","\n","# 4. Configuração das Variáveis de Ambiente\n","# Define as variáveis de ambiente para que o sistema operacional e o Python saibam onde encontrar as instalações do Java e do Spark.\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\"\n","\n","# 5. Instalação das bibliotecas Python para Spark\n","# Instala as bibliotecas Python necessárias para interagir com o Spark.\n","# !pip install -q: Instala os pacotes usando o gerenciador de pacotes do Python (pip) em modo silencioso.\n","#   pyspark==3.5.1: A biblioteca que fornece a API Python para o Spark. A versão é fixada para corresponder à versão do Spark baixada.\n","#   findspark: Uma biblioteca útil que ajuda o Python a localizar a instalação do Spark no sistema.\n","!pip install -q pyspark==3.5.1 findspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CohocGNtPlEx","outputId":"b62082a8-2446-4682-b322-448614701761","executionInfo":{"status":"ok","timestamp":1750531370816,"user_tz":180,"elapsed":59485,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","openjdk-11-jdk-headless is already the newest version (11.0.27+6~us1-0ubuntu1~22.04).\n","0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n","spark-3.5.1-bin-hadoop3/\n","spark-3.5.1-bin-hadoop3/sbin/\n","spark-3.5.1-bin-hadoop3/sbin/spark-config.sh\n","spark-3.5.1-bin-hadoop3/sbin/stop-slave.sh\n","spark-3.5.1-bin-hadoop3/sbin/stop-mesos-dispatcher.sh\n","spark-3.5.1-bin-hadoop3/sbin/start-workers.sh\n","spark-3.5.1-bin-hadoop3/sbin/start-slaves.sh\n","spark-3.5.1-bin-hadoop3/sbin/start-all.sh\n","spark-3.5.1-bin-hadoop3/sbin/stop-all.sh\n","spark-3.5.1-bin-hadoop3/sbin/workers.sh\n","spark-3.5.1-bin-hadoop3/sbin/start-mesos-dispatcher.sh\n","spark-3.5.1-bin-hadoop3/sbin/spark-daemon.sh\n","spark-3.5.1-bin-hadoop3/sbin/decommission-worker.sh\n","spark-3.5.1-bin-hadoop3/sbin/slaves.sh\n","spark-3.5.1-bin-hadoop3/sbin/stop-mesos-shuffle-service.sh\n","spark-3.5.1-bin-hadoop3/sbin/stop-history-server.sh\n","spark-3.5.1-bin-hadoop3/sbin/stop-worker.sh\n","spark-3.5.1-bin-hadoop3/sbin/decommission-slave.sh\n","spark-3.5.1-bin-hadoop3/sbin/stop-thriftserver.sh\n","spark-3.5.1-bin-hadoop3/sbin/start-worker.sh\n","spark-3.5.1-bin-hadoop3/sbin/stop-slaves.sh\n","spark-3.5.1-bin-hadoop3/sbin/start-connect-server.sh\n","spark-3.5.1-bin-hadoop3/sbin/start-thriftserver.sh\n","spark-3.5.1-bin-hadoop3/sbin/start-history-server.sh\n","spark-3.5.1-bin-hadoop3/sbin/stop-connect-server.sh\n","spark-3.5.1-bin-hadoop3/sbin/spark-daemons.sh\n","spark-3.5.1-bin-hadoop3/sbin/start-slave.sh\n","spark-3.5.1-bin-hadoop3/sbin/start-master.sh\n","spark-3.5.1-bin-hadoop3/sbin/start-mesos-shuffle-service.sh\n","spark-3.5.1-bin-hadoop3/sbin/stop-master.sh\n","spark-3.5.1-bin-hadoop3/sbin/stop-workers.sh\n","spark-3.5.1-bin-hadoop3/licenses/\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-automaton.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-d3.min.js.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-scopt.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-leveldbjni.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-blas.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-spire.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-paranamer.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-matchMedia-polyfill.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-jaxb-runtime.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-zstd-jni.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-pyrolite.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-dagre-d3.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-javax-transaction-transaction-api.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-json-formatter.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-jsp-api.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-arpack.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-AnchorJS.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-JLargeArrays.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-protobuf.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-pmml-model.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-slf4j.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-py4j.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-datatables.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-join.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-bootstrap.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-jakarta.activation-api.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-jline.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-vis-timeline.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-jakarta-annotation-api\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-zstd.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-respond.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-jakarta-ws-rs-api\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-graphlib-dot.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-modernizr.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-kryo.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-dnsjava.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-cloudpickle.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-janino.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-sbt-launch-lib.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-javolution.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-mustache.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-f2j.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-jodd.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-machinist.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-JTransforms.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-CC0.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-jakarta.xml.bind-api.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-reflectasm.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-sorttable.js.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-istack-commons-runtime.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-minlog.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-jquery.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-antlr.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-xmlenc.txt\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-javassist.html\n","spark-3.5.1-bin-hadoop3/licenses/LICENSE-re2j.txt\n","spark-3.5.1-bin-hadoop3/data/\n","spark-3.5.1-bin-hadoop3/data/graphx/\n","spark-3.5.1-bin-hadoop3/data/graphx/followers.txt\n","spark-3.5.1-bin-hadoop3/data/graphx/users.txt\n","spark-3.5.1-bin-hadoop3/data/streaming/\n","spark-3.5.1-bin-hadoop3/data/streaming/AFINN-111.txt\n","spark-3.5.1-bin-hadoop3/data/mllib/\n","spark-3.5.1-bin-hadoop3/data/mllib/sample_multiclass_classification_data.txt\n","spark-3.5.1-bin-hadoop3/data/mllib/sample_svm_data.txt\n","spark-3.5.1-bin-hadoop3/data/mllib/kmeans_data.txt\n","spark-3.5.1-bin-hadoop3/data/mllib/streaming_kmeans_data_test.txt\n","spark-3.5.1-bin-hadoop3/data/mllib/sample_movielens_data.txt\n","spark-3.5.1-bin-hadoop3/data/mllib/images/\n","spark-3.5.1-bin-hadoop3/data/mllib/images/origin/\n","spark-3.5.1-bin-hadoop3/data/mllib/images/origin/kittens/\n","spark-3.5.1-bin-hadoop3/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n","spark-3.5.1-bin-hadoop3/data/mllib/images/origin/kittens/54893.jpg\n","spark-3.5.1-bin-hadoop3/data/mllib/images/origin/kittens/DP802813.jpg\n","spark-3.5.1-bin-hadoop3/data/mllib/images/origin/kittens/not-image.txt\n","spark-3.5.1-bin-hadoop3/data/mllib/images/origin/kittens/DP153539.jpg\n","spark-3.5.1-bin-hadoop3/data/mllib/images/origin/multi-channel/\n","spark-3.5.1-bin-hadoop3/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n","spark-3.5.1-bin-hadoop3/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n","spark-3.5.1-bin-hadoop3/data/mllib/images/origin/multi-channel/BGRA.png\n","spark-3.5.1-bin-hadoop3/data/mllib/images/origin/multi-channel/grayscale.jpg\n","spark-3.5.1-bin-hadoop3/data/mllib/images/origin/license.txt\n","spark-3.5.1-bin-hadoop3/data/mllib/images/license.txt\n","spark-3.5.1-bin-hadoop3/data/mllib/ridge-data/\n","spark-3.5.1-bin-hadoop3/data/mllib/ridge-data/lpsa.data\n","spark-3.5.1-bin-hadoop3/data/mllib/als/\n","spark-3.5.1-bin-hadoop3/data/mllib/als/sample_movielens_ratings.txt\n","spark-3.5.1-bin-hadoop3/data/mllib/als/test.data\n","spark-3.5.1-bin-hadoop3/data/mllib/sample_binary_classification_data.txt\n","spark-3.5.1-bin-hadoop3/data/mllib/gmm_data.txt\n","spark-3.5.1-bin-hadoop3/data/mllib/sample_libsvm_data.txt\n","spark-3.5.1-bin-hadoop3/data/mllib/pagerank_data.txt\n","spark-3.5.1-bin-hadoop3/data/mllib/sample_isotonic_regression_libsvm_data.txt\n","spark-3.5.1-bin-hadoop3/data/mllib/sample_linear_regression_data.txt\n","spark-3.5.1-bin-hadoop3/data/mllib/sample_lda_data.txt\n","spark-3.5.1-bin-hadoop3/data/mllib/sample_fpgrowth.txt\n","spark-3.5.1-bin-hadoop3/data/mllib/sample_kmeans_data.txt\n","spark-3.5.1-bin-hadoop3/data/mllib/sample_lda_libsvm_data.txt\n","spark-3.5.1-bin-hadoop3/data/mllib/pic_data.txt\n","spark-3.5.1-bin-hadoop3/data/artifact-tests/\n","spark-3.5.1-bin-hadoop3/data/artifact-tests/smallJar.jar\n","spark-3.5.1-bin-hadoop3/data/artifact-tests/junitLargeJar.jar\n","spark-3.5.1-bin-hadoop3/data/artifact-tests/crc/\n","spark-3.5.1-bin-hadoop3/data/artifact-tests/crc/smallJar.txt\n","spark-3.5.1-bin-hadoop3/data/artifact-tests/crc/junitLargeJar.txt\n","spark-3.5.1-bin-hadoop3/data/artifact-tests/crc/README.md\n","spark-3.5.1-bin-hadoop3/jars/\n","spark-3.5.1-bin-hadoop3/jars/jersey-container-servlet-2.40.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-sketch_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/jackson-annotations-2.15.2.jar\n","spark-3.5.1-bin-hadoop3/jars/netty-transport-native-epoll-4.1.96.Final-linux-x86_64.jar\n","spark-3.5.1-bin-hadoop3/jars/spire-macros_2.12-0.17.0.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-extensions-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/jline-2.14.6.jar\n","spark-3.5.1-bin-hadoop3/jars/orc-mapreduce-1.9.2-shaded-protobuf.jar\n","spark-3.5.1-bin-hadoop3/jars/HikariCP-2.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-storageclass-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/orc-core-1.9.2-shaded-protobuf.jar\n","spark-3.5.1-bin-hadoop3/jars/objenesis-3.3.jar\n","spark-3.5.1-bin-hadoop3/jars/snakeyaml-2.0.jar\n","spark-3.5.1-bin-hadoop3/jars/parquet-hadoop-1.13.1.jar\n","spark-3.5.1-bin-hadoop3/jars/httpclient-4.5.14.jar\n","spark-3.5.1-bin-hadoop3/jars/curator-client-2.13.0.jar\n","spark-3.5.1-bin-hadoop3/jars/json4s-core_2.12-3.7.0-M11.jar\n","spark-3.5.1-bin-hadoop3/jars/netty-transport-classes-epoll-4.1.96.Final.jar\n","spark-3.5.1-bin-hadoop3/jars/netty-transport-native-kqueue-4.1.96.Final-osx-aarch_64.jar\n","spark-3.5.1-bin-hadoop3/jars/commons-io-2.13.0.jar\n","spark-3.5.1-bin-hadoop3/jars/jackson-databind-2.15.2.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-network-common_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/hadoop-yarn-server-web-proxy-3.3.4.jar\n","spark-3.5.1-bin-hadoop3/jars/netty-codec-socks-4.1.96.Final.jar\n","spark-3.5.1-bin-hadoop3/jars/hadoop-client-api-3.3.4.jar\n","spark-3.5.1-bin-hadoop3/jars/curator-recipes-2.13.0.jar\n","spark-3.5.1-bin-hadoop3/jars/RoaringBitmap-0.9.45.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-mllib-local_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-hive-thriftserver_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/commons-math3-3.6.1.jar\n","spark-3.5.1-bin-hadoop3/jars/janino-3.1.9.jar\n","spark-3.5.1-bin-hadoop3/jars/datanucleus-rdbms-4.1.19.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-catalyst_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/hive-exec-2.3.9-core.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-mesos_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/gson-2.2.4.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-node-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/commons-cli-1.5.0.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-launcher_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/shims-0.9.45.jar\n","spark-3.5.1-bin-hadoop3/jars/spire_2.12-0.17.0.jar\n","spark-3.5.1-bin-hadoop3/jars/netty-handler-proxy-4.1.96.Final.jar\n","spark-3.5.1-bin-hadoop3/jars/algebra_2.12-2.0.1.jar\n","spark-3.5.1-bin-hadoop3/jars/log4j-slf4j2-impl-2.20.0.jar\n","spark-3.5.1-bin-hadoop3/jars/json4s-scalap_2.12-3.7.0-M11.jar\n","spark-3.5.1-bin-hadoop3/jars/ST4-4.0.4.jar\n","spark-3.5.1-bin-hadoop3/jars/aircompressor-0.26.jar\n","spark-3.5.1-bin-hadoop3/jars/parquet-column-1.13.1.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-kubernetes_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-scheduling-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/snappy-java-1.1.10.3.jar\n","spark-3.5.1-bin-hadoop3/jars/transaction-api-1.1.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-common-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/jackson-core-2.15.2.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-certificates-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/jakarta.ws.rs-api-2.1.6.jar\n","spark-3.5.1-bin-hadoop3/jars/lapack-3.0.3.jar\n","spark-3.5.1-bin-hadoop3/jars/netty-codec-http2-4.1.96.Final.jar\n","spark-3.5.1-bin-hadoop3/jars/netty-transport-native-epoll-4.1.96.Final-linux-aarch_64.jar\n","spark-3.5.1-bin-hadoop3/jars/JTransforms-3.1.jar\n","spark-3.5.1-bin-hadoop3/jars/hive-llap-common-2.3.9.jar\n","spark-3.5.1-bin-hadoop3/jars/json-1.8.jar\n","spark-3.5.1-bin-hadoop3/jars/hive-storage-api-2.8.1.jar\n","spark-3.5.1-bin-hadoop3/jars/jsr305-3.0.0.jar\n","spark-3.5.1-bin-hadoop3/jars/parquet-common-1.13.1.jar\n","spark-3.5.1-bin-hadoop3/jars/oro-2.0.8.jar\n","spark-3.5.1-bin-hadoop3/jars/cats-kernel_2.12-2.1.1.jar\n","spark-3.5.1-bin-hadoop3/jars/breeze-macros_2.12-2.1.0.jar\n","spark-3.5.1-bin-hadoop3/jars/py4j-0.10.9.7.jar\n","spark-3.5.1-bin-hadoop3/jars/commons-collections4-4.4.jar\n","spark-3.5.1-bin-hadoop3/jars/javolution-5.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/slf4j-api-2.0.7.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-network-shuffle_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/tink-1.9.0.jar\n","spark-3.5.1-bin-hadoop3/jars/arrow-memory-netty-12.0.1.jar\n","spark-3.5.1-bin-hadoop3/jars/avro-1.11.2.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-networking-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-metrics-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/leveldbjni-all-1.8.jar\n","spark-3.5.1-bin-hadoop3/jars/jackson-core-asl-1.9.13.jar\n","spark-3.5.1-bin-hadoop3/jars/hive-shims-common-2.3.9.jar\n","spark-3.5.1-bin-hadoop3/jars/audience-annotations-0.5.0.jar\n","spark-3.5.1-bin-hadoop3/jars/scala-parser-combinators_2.12-2.3.0.jar\n","spark-3.5.1-bin-hadoop3/jars/pickle-1.3.jar\n","spark-3.5.1-bin-hadoop3/jars/hive-service-rpc-3.1.3.jar\n","spark-3.5.1-bin-hadoop3/jars/netty-transport-4.1.96.Final.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-batch-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/spire-platform_2.12-0.17.0.jar\n","spark-3.5.1-bin-hadoop3/jars/hive-common-2.3.9.jar\n","spark-3.5.1-bin-hadoop3/jars/blas-3.0.3.jar\n","spark-3.5.1-bin-hadoop3/jars/netty-codec-4.1.96.Final.jar\n","spark-3.5.1-bin-hadoop3/jars/log4j-api-2.20.0.jar\n","spark-3.5.1-bin-hadoop3/jars/commons-dbcp-1.4.jar\n","spark-3.5.1-bin-hadoop3/jars/chill_2.12-0.10.0.jar\n","spark-3.5.1-bin-hadoop3/jars/commons-compiler-3.1.9.jar\n","spark-3.5.1-bin-hadoop3/jars/jersey-common-2.40.jar\n","spark-3.5.1-bin-hadoop3/jars/metrics-json-4.2.19.jar\n","spark-3.5.1-bin-hadoop3/jars/commons-codec-1.16.0.jar\n","spark-3.5.1-bin-hadoop3/jars/kryo-shaded-4.0.2.jar\n","spark-3.5.1-bin-hadoop3/jars/netty-transport-native-kqueue-4.1.96.Final-osx-x86_64.jar\n","spark-3.5.1-bin-hadoop3/jars/univocity-parsers-2.9.1.jar\n","spark-3.5.1-bin-hadoop3/jars/json4s-jackson_2.12-3.7.0-M11.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-client-api-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/okio-1.15.0.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-resource-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/mesos-1.4.3-shaded-protobuf.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-apiextensions-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/hive-metastore-2.3.9.jar\n","spark-3.5.1-bin-hadoop3/jars/scala-reflect-2.12.18.jar\n","spark-3.5.1-bin-hadoop3/jars/flatbuffers-java-1.12.0.jar\n","spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/jersey-hk2-2.40.jar\n","spark-3.5.1-bin-hadoop3/jars/breeze_2.12-2.1.0.jar\n","spark-3.5.1-bin-hadoop3/jars/netty-common-4.1.96.Final.jar\n","spark-3.5.1-bin-hadoop3/jars/parquet-jackson-1.13.1.jar\n","spark-3.5.1-bin-hadoop3/jars/hk2-api-2.6.1.jar\n","spark-3.5.1-bin-hadoop3/jars/arpack-3.0.3.jar\n","spark-3.5.1-bin-hadoop3/jars/scala-xml_2.12-2.1.0.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-core_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/lz4-java-1.8.0.jar\n","spark-3.5.1-bin-hadoop3/jars/joda-time-2.12.5.jar\n","spark-3.5.1-bin-hadoop3/jars/metrics-jvm-4.2.19.jar\n","spark-3.5.1-bin-hadoop3/jars/log4j-1.2-api-2.20.0.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-coordination-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/jersey-client-2.40.jar\n","spark-3.5.1-bin-hadoop3/jars/json4s-ast_2.12-3.7.0-M11.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-apps-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/commons-lang-2.6.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-gatewayapi-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/jackson-mapper-asl-1.9.13.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-common-utils_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/zstd-jni-1.5.5-4.jar\n","spark-3.5.1-bin-hadoop3/jars/jodd-core-3.5.2.jar\n","spark-3.5.1-bin-hadoop3/jars/jpam-1.1.jar\n","spark-3.5.1-bin-hadoop3/jars/metrics-core-4.2.19.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-unsafe_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/jakarta.servlet-api-4.0.3.jar\n","spark-3.5.1-bin-hadoop3/jars/parquet-format-structures-1.13.1.jar\n","spark-3.5.1-bin-hadoop3/jars/log4j-core-2.20.0.jar\n","spark-3.5.1-bin-hadoop3/jars/rocksdbjni-8.3.2.jar\n","spark-3.5.1-bin-hadoop3/jars/hive-shims-scheduler-2.3.9.jar\n","spark-3.5.1-bin-hadoop3/jars/netty-codec-http-4.1.96.Final.jar\n","spark-3.5.1-bin-hadoop3/jars/paranamer-2.8.jar\n","spark-3.5.1-bin-hadoop3/jars/hive-shims-0.23-2.3.9.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-autoscaling-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/scala-collection-compat_2.12-2.7.0.jar\n","spark-3.5.1-bin-hadoop3/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\n","spark-3.5.1-bin-hadoop3/jars/curator-framework-2.13.0.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-mllib_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/opencsv-2.3.jar\n","spark-3.5.1-bin-hadoop3/jars/netty-buffer-4.1.96.Final.jar\n","spark-3.5.1-bin-hadoop3/jars/scala-library-2.12.18.jar\n","spark-3.5.1-bin-hadoop3/jars/xz-1.9.jar\n","spark-3.5.1-bin-hadoop3/jars/commons-collections-3.2.2.jar\n","spark-3.5.1-bin-hadoop3/jars/jackson-dataformat-yaml-2.15.2.jar\n","spark-3.5.1-bin-hadoop3/jars/datanucleus-core-4.1.17.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-repl_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/bonecp-0.8.0.RELEASE.jar\n","spark-3.5.1-bin-hadoop3/jars/guava-14.0.1.jar\n","spark-3.5.1-bin-hadoop3/jars/zjsonpatch-0.3.0.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-sql-api_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/netty-all-4.1.96.Final.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-streaming_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/arrow-format-12.0.1.jar\n","spark-3.5.1-bin-hadoop3/jars/libthrift-0.12.0.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-hive_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/netty-resolver-4.1.96.Final.jar\n","spark-3.5.1-bin-hadoop3/jars/hadoop-shaded-guava-1.1.1.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-httpclient-okhttp-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/hive-shims-2.3.9.jar\n","spark-3.5.1-bin-hadoop3/jars/metrics-jmx-4.2.19.jar\n","spark-3.5.1-bin-hadoop3/jars/hive-beeline-2.3.9.jar\n","spark-3.5.1-bin-hadoop3/jars/jakarta.annotation-api-1.3.5.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-rbac-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/metrics-graphite-4.2.19.jar\n","spark-3.5.1-bin-hadoop3/jars/activation-1.1.1.jar\n","spark-3.5.1-bin-hadoop3/jars/jcl-over-slf4j-2.0.7.jar\n","spark-3.5.1-bin-hadoop3/jars/okhttp-3.12.12.jar\n","spark-3.5.1-bin-hadoop3/jars/httpcore-4.4.16.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-flowcontrol-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-tags_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/commons-text-1.10.0.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-client-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/javassist-3.29.2-GA.jar\n","spark-3.5.1-bin-hadoop3/jars/parquet-encoding-1.13.1.jar\n","spark-3.5.1-bin-hadoop3/jars/commons-pool-1.5.4.jar\n","spark-3.5.1-bin-hadoop3/jars/zookeeper-jute-3.6.3.jar\n","spark-3.5.1-bin-hadoop3/jars/xbean-asm9-shaded-4.23.jar\n","spark-3.5.1-bin-hadoop3/jars/netty-transport-native-unix-common-4.1.96.Final.jar\n","spark-3.5.1-bin-hadoop3/jars/minlog-1.3.0.jar\n","spark-3.5.1-bin-hadoop3/jars/avro-mapred-1.11.2.jar\n","spark-3.5.1-bin-hadoop3/jars/stax-api-1.0.1.jar\n","spark-3.5.1-bin-hadoop3/jars/jakarta.xml.bind-api-2.3.2.jar\n","spark-3.5.1-bin-hadoop3/jars/jackson-module-scala_2.12-2.15.2.jar\n","spark-3.5.1-bin-hadoop3/jars/arrow-vector-12.0.1.jar\n","spark-3.5.1-bin-hadoop3/jars/antlr-runtime-3.5.2.jar\n","spark-3.5.1-bin-hadoop3/jars/snakeyaml-engine-2.6.jar\n","spark-3.5.1-bin-hadoop3/jars/hk2-locator-2.6.1.jar\n","spark-3.5.1-bin-hadoop3/jars/commons-logging-1.1.3.jar\n","spark-3.5.1-bin-hadoop3/jars/hive-jdbc-2.3.9.jar\n","spark-3.5.1-bin-hadoop3/jars/hive-cli-2.3.9.jar\n","spark-3.5.1-bin-hadoop3/jars/arrow-memory-core-12.0.1.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-discovery-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/logging-interceptor-3.12.12.jar\n","spark-3.5.1-bin-hadoop3/jars/JLargeArrays-1.5.jar\n","spark-3.5.1-bin-hadoop3/jars/derby-10.14.2.0.jar\n","spark-3.5.1-bin-hadoop3/jars/hadoop-client-runtime-3.3.4.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-kvstore_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/commons-compress-1.23.0.jar\n","spark-3.5.1-bin-hadoop3/jars/jul-to-slf4j-2.0.7.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-graphx_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-admissionregistration-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/commons-lang3-3.12.0.jar\n","spark-3.5.1-bin-hadoop3/jars/hive-serde-2.3.9.jar\n","spark-3.5.1-bin-hadoop3/jars/javax.jdo-3.2.0-m3.jar\n","spark-3.5.1-bin-hadoop3/jars/arpack_combined_all-0.1.jar\n","spark-3.5.1-bin-hadoop3/jars/super-csv-2.2.0.jar\n","spark-3.5.1-bin-hadoop3/jars/hk2-utils-2.6.1.jar\n","spark-3.5.1-bin-hadoop3/jars/jackson-datatype-jsr310-2.15.2.jar\n","spark-3.5.1-bin-hadoop3/jars/antlr4-runtime-4.9.3.jar\n","spark-3.5.1-bin-hadoop3/jars/stream-2.9.6.jar\n","spark-3.5.1-bin-hadoop3/jars/threeten-extra-1.7.1.jar\n","spark-3.5.1-bin-hadoop3/jars/spire-util_2.12-0.17.0.jar\n","spark-3.5.1-bin-hadoop3/jars/jakarta.validation-api-2.0.2.jar\n","spark-3.5.1-bin-hadoop3/jars/jaxb-runtime-2.3.2.jar\n","spark-3.5.1-bin-hadoop3/jars/scala-compiler-2.12.18.jar\n","spark-3.5.1-bin-hadoop3/jars/datasketches-memory-2.1.0.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-yarn_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-events-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-core-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/jars/aopalliance-repackaged-2.6.1.jar\n","spark-3.5.1-bin-hadoop3/jars/avro-ipc-1.11.2.jar\n","spark-3.5.1-bin-hadoop3/jars/jta-1.1.jar\n","spark-3.5.1-bin-hadoop3/jars/compress-lzf-1.1.2.jar\n","spark-3.5.1-bin-hadoop3/jars/annotations-17.0.0.jar\n","spark-3.5.1-bin-hadoop3/jars/datanucleus-api-jdo-4.2.4.jar\n","spark-3.5.1-bin-hadoop3/jars/jersey-server-2.40.jar\n","spark-3.5.1-bin-hadoop3/jars/commons-crypto-1.1.0.jar\n","spark-3.5.1-bin-hadoop3/jars/jdo-api-3.0.1.jar\n","spark-3.5.1-bin-hadoop3/jars/jersey-container-servlet-core-2.40.jar\n","spark-3.5.1-bin-hadoop3/jars/netty-handler-4.1.96.Final.jar\n","spark-3.5.1-bin-hadoop3/jars/jakarta.inject-2.6.1.jar\n","spark-3.5.1-bin-hadoop3/jars/chill-java-0.10.0.jar\n","spark-3.5.1-bin-hadoop3/jars/netty-transport-classes-kqueue-4.1.96.Final.jar\n","spark-3.5.1-bin-hadoop3/jars/istack-commons-runtime-3.0.8.jar\n","spark-3.5.1-bin-hadoop3/jars/spark-sql_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/jars/zookeeper-3.6.3.jar\n","spark-3.5.1-bin-hadoop3/jars/orc-shims-1.9.2.jar\n","spark-3.5.1-bin-hadoop3/jars/datasketches-java-3.3.0.jar\n","spark-3.5.1-bin-hadoop3/jars/libfb303-0.9.3.jar\n","spark-3.5.1-bin-hadoop3/jars/osgi-resource-locator-1.0.3.jar\n","spark-3.5.1-bin-hadoop3/jars/kubernetes-model-policy-6.7.2.jar\n","spark-3.5.1-bin-hadoop3/examples/\n","spark-3.5.1-bin-hadoop3/examples/jars/\n","spark-3.5.1-bin-hadoop3/examples/jars/spark-examples_2.12-3.5.1.jar\n","spark-3.5.1-bin-hadoop3/examples/jars/scopt_2.12-3.7.1.jar\n","spark-3.5.1-bin-hadoop3/examples/src/\n","spark-3.5.1-bin-hadoop3/examples/src/main/\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKerberizedKafkaWordCount.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/MiniReadWriteTest.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VarianceThresholdSelectorExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RobustScalerExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/UnivariateFeatureSelectorExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/FMClassifierExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/FMRegressorExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKerberizedKafkaWordCount.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredComplexSessionization.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/SimpleTypedAggregator.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/jdbc/\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/jdbc/ExampleJdbcConnectionProvider.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedScalar.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/SessionExtensionsWithoutLoader.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/SparkSessionExtensionsTest.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/AgeExample.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/SessionExtensionsWithLoader.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/AccumulatorMetricsTest.scala\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/streaming/\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/streaming/structured_network_wordcount.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/lm_with_elastic_net.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/kstest.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/fmRegressor.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/survreg.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/randomForest.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/prefixSpan.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/kmeans.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/powerIterationClustering.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/fpm.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/gbt.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/fmClassifier.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/logit.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/als.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/glm.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/bisectingKmeans.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/decisionTree.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/ml.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/naiveBayes.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/isoreg.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/svmLinear.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/mlp.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/gaussianMixture.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/ml/lda.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/dataframe.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/RSparkSQLExample.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/r/data-manipulation.R\n","spark-3.5.1-bin-hadoop3/examples/src/main/scripts/\n","spark-3.5.1-bin-hadoop3/examples/src/main/scripts/getGpusResources.sh\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKerberizedKafkaWordCount.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaRobustScalerExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaFMClassifierExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaUnivariateFeatureSelectorExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVarianceThresholdSelectorExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaFMRegressorExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKerberizedKafkaWordCount.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredComplexSessionization.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedScalar.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/hive/\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n","spark-3.5.1-bin-hadoop3/examples/src/main/resources/\n","spark-3.5.1-bin-hadoop3/examples/src/main/resources/dir1/\n","spark-3.5.1-bin-hadoop3/examples/src/main/resources/dir1/file1.parquet\n","spark-3.5.1-bin-hadoop3/examples/src/main/resources/dir1/file3.json\n","spark-3.5.1-bin-hadoop3/examples/src/main/resources/dir1/dir2/\n","spark-3.5.1-bin-hadoop3/examples/src/main/resources/dir1/dir2/file2.parquet\n","spark-3.5.1-bin-hadoop3/examples/src/main/resources/users.avro\n","spark-3.5.1-bin-hadoop3/examples/src/main/resources/users.parquet\n","spark-3.5.1-bin-hadoop3/examples/src/main/resources/META-INF/\n","spark-3.5.1-bin-hadoop3/examples/src/main/resources/META-INF/services/\n","spark-3.5.1-bin-hadoop3/examples/src/main/resources/META-INF/services/org.apache.spark.sql.SparkSessionExtensionsProvider\n","spark-3.5.1-bin-hadoop3/examples/src/main/resources/META-INF/services/org.apache.spark.sql.jdbc.JdbcConnectionProvider\n","spark-3.5.1-bin-hadoop3/examples/src/main/resources/people.txt\n","spark-3.5.1-bin-hadoop3/examples/src/main/resources/employees.json\n","spark-3.5.1-bin-hadoop3/examples/src/main/resources/kv1.txt\n","spark-3.5.1-bin-hadoop3/examples/src/main/resources/user.avsc\n","spark-3.5.1-bin-hadoop3/examples/src/main/resources/users.orc\n","spark-3.5.1-bin-hadoop3/examples/src/main/resources/people.json\n","spark-3.5.1-bin-hadoop3/examples/src/main/resources/full_user.avsc\n","spark-3.5.1-bin-hadoop3/examples/src/main/resources/people.csv\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/streaming/\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/streaming/hdfs_wordcount.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/streaming/sql_network_wordcount.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/streaming/network_wordcount.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/streaming/recoverable_network_wordcount.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/streaming/stateful_network_wordcount.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/streaming/__init__.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/streaming/network_wordjoinsentiments.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/streaming/queue_stream.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/avro_inputformat.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/transitive_closure.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/stopwords_remover_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/pipeline_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/vector_assembler_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/__init__,py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/cross_validator.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/tf_idf_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/summarizer_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/string_indexer_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/decision_tree_regression_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/sql_transformer.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/interaction_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/count_vectorizer_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/dct_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/power_iteration_clustering_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/quantile_discretizer_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/bucketizer_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/rformula_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/isotonic_regression_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/tokenizer_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/one_vs_rest_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/lda_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/chisq_selector_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/feature_hasher_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/estimator_transformer_param_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/univariate_feature_selector_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/imputer_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/vector_slicer_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/chi_square_test_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/decision_tree_classification_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/min_max_scaler_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/prefixspan_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/standard_scaler_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/aft_survival_regression.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/bisecting_k_means_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/vector_indexer_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/robust_scaler_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/vector_size_hint_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/n_gram_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/als_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/binarizer_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/variance_threshold_selector_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/fm_regressor_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/kmeans_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/normalizer_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/dataframe_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/max_abs_scaler_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/elementwise_product_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/linearsvc.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/random_forest_classifier_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/correlation_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/multilayer_perceptron_classification.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/naive_bayes_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/fm_classifier_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/logistic_regression_summary_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/index_to_string_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/onehot_encoder_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/polynomial_expansion_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/train_validation_split.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/gaussian_mixture_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/min_hash_lsh_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/pca_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/fpgrowth_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/random_forest_regressor_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/word2vec_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/ml/generalized_linear_regression_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/sql/\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/sql/datasource.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/sql/streaming/\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/sql/streaming/__init__,py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/sql/streaming/structured_network_wordcount_session_window.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/sql/streaming/structured_sessionization.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/sql/arrow.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/sql/udtf.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/sql/basic.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/sql/hive.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/sql/__init__.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/wordcount.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/pagerank.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/kmeans.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/tf_idf_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/kernel_density_estimation_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/decision_tree_regression_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/random_forest_regression_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/random_forest_classification_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/sampled_rdds.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/recommendation_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/power_iteration_clustering_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/isotonic_regression_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/correlations_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/ranking_metrics_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/binary_classification_metrics_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/hypothesis_testing_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/regression_metrics_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/decision_tree_classification_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/random_rdd_generation.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/multi_label_metrics_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/streaming_linear_regression_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/summary_statistics_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/standard_scaler_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/bisecting_k_means_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/stratified_sampling_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/kmeans.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/word2vec.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/gaussian_mixture_model.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/normalizer_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/svd_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/pca_rowmatrix_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/elementwise_product_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/k_means_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/naive_bayes_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/multi_class_metrics_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/svm_with_sgd_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/__init__.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/streaming_k_means_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/gaussian_mixture_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/correlations.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/logistic_regression.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/fpgrowth_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/mllib/word2vec_example.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/sort.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/pi.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/als.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/__init__.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/status_api_demo.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/logistic_regression.py\n","spark-3.5.1-bin-hadoop3/examples/src/main/python/parquet_inputformat.py\n","spark-3.5.1-bin-hadoop3/NOTICE\n","spark-3.5.1-bin-hadoop3/conf/\n","spark-3.5.1-bin-hadoop3/conf/log4j2.properties.template\n","spark-3.5.1-bin-hadoop3/conf/workers.template\n","spark-3.5.1-bin-hadoop3/conf/spark-env.sh.template\n","spark-3.5.1-bin-hadoop3/conf/metrics.properties.template\n","spark-3.5.1-bin-hadoop3/conf/spark-defaults.conf.template\n","spark-3.5.1-bin-hadoop3/conf/fairscheduler.xml.template\n","spark-3.5.1-bin-hadoop3/bin/\n","spark-3.5.1-bin-hadoop3/bin/run-example\n","spark-3.5.1-bin-hadoop3/bin/spark-shell.cmd\n","spark-3.5.1-bin-hadoop3/bin/spark-class\n","spark-3.5.1-bin-hadoop3/bin/sparkR\n","spark-3.5.1-bin-hadoop3/bin/beeline\n","spark-3.5.1-bin-hadoop3/bin/spark-shell2.cmd\n","spark-3.5.1-bin-hadoop3/bin/find-spark-home\n","spark-3.5.1-bin-hadoop3/bin/pyspark.cmd\n","spark-3.5.1-bin-hadoop3/bin/spark-submit2.cmd\n","spark-3.5.1-bin-hadoop3/bin/pyspark2.cmd\n","spark-3.5.1-bin-hadoop3/bin/spark-connect-shell\n","spark-3.5.1-bin-hadoop3/bin/docker-image-tool.sh\n","spark-3.5.1-bin-hadoop3/bin/load-spark-env.cmd\n","spark-3.5.1-bin-hadoop3/bin/spark-sql.cmd\n","spark-3.5.1-bin-hadoop3/bin/find-spark-home.cmd\n","spark-3.5.1-bin-hadoop3/bin/beeline.cmd\n","spark-3.5.1-bin-hadoop3/bin/spark-class.cmd\n","spark-3.5.1-bin-hadoop3/bin/sparkR2.cmd\n","spark-3.5.1-bin-hadoop3/bin/spark-sql2.cmd\n","spark-3.5.1-bin-hadoop3/bin/spark-sql\n","spark-3.5.1-bin-hadoop3/bin/sparkR.cmd\n","spark-3.5.1-bin-hadoop3/bin/spark-submit\n","spark-3.5.1-bin-hadoop3/bin/spark-shell\n","spark-3.5.1-bin-hadoop3/bin/spark-submit.cmd\n","spark-3.5.1-bin-hadoop3/bin/pyspark\n","spark-3.5.1-bin-hadoop3/bin/run-example.cmd\n","spark-3.5.1-bin-hadoop3/bin/load-spark-env.sh\n","spark-3.5.1-bin-hadoop3/bin/spark-class2.cmd\n","spark-3.5.1-bin-hadoop3/kubernetes/\n","spark-3.5.1-bin-hadoop3/kubernetes/tests/\n","spark-3.5.1-bin-hadoop3/kubernetes/tests/decommissioning.py\n","spark-3.5.1-bin-hadoop3/kubernetes/tests/autoscale.py\n","spark-3.5.1-bin-hadoop3/kubernetes/tests/python_executable_check.py\n","spark-3.5.1-bin-hadoop3/kubernetes/tests/worker_memory_check.py\n","spark-3.5.1-bin-hadoop3/kubernetes/tests/py_container_checks.py\n","spark-3.5.1-bin-hadoop3/kubernetes/tests/pyfiles.py\n","spark-3.5.1-bin-hadoop3/kubernetes/tests/decommissioning_cleanup.py\n","spark-3.5.1-bin-hadoop3/kubernetes/dockerfiles/\n","spark-3.5.1-bin-hadoop3/kubernetes/dockerfiles/spark/\n","spark-3.5.1-bin-hadoop3/kubernetes/dockerfiles/spark/decom.sh\n","spark-3.5.1-bin-hadoop3/kubernetes/dockerfiles/spark/Dockerfile\n","spark-3.5.1-bin-hadoop3/kubernetes/dockerfiles/spark/entrypoint.sh\n","spark-3.5.1-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/\n","spark-3.5.1-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/python/\n","spark-3.5.1-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/python/Dockerfile\n","spark-3.5.1-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/R/\n","spark-3.5.1-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/R/Dockerfile\n","spark-3.5.1-bin-hadoop3/README.md\n","spark-3.5.1-bin-hadoop3/LICENSE\n","spark-3.5.1-bin-hadoop3/yarn/\n","spark-3.5.1-bin-hadoop3/yarn/spark-3.5.1-yarn-shuffle.jar\n","spark-3.5.1-bin-hadoop3/python/\n","spark-3.5.1-bin-hadoop3/python/setup.py\n","spark-3.5.1-bin-hadoop3/python/MANIFEST.in\n","spark-3.5.1-bin-hadoop3/python/test_coverage/\n","spark-3.5.1-bin-hadoop3/python/test_coverage/sitecustomize.py\n","spark-3.5.1-bin-hadoop3/python/test_coverage/conf/\n","spark-3.5.1-bin-hadoop3/python/test_coverage/conf/spark-defaults.conf\n","spark-3.5.1-bin-hadoop3/python/test_coverage/coverage_daemon.py\n","spark-3.5.1-bin-hadoop3/python/test_support/\n","spark-3.5.1-bin-hadoop3/python/test_support/hello/\n","spark-3.5.1-bin-hadoop3/python/test_support/hello/hello.txt\n","spark-3.5.1-bin-hadoop3/python/test_support/hello/sub_hello/\n","spark-3.5.1-bin-hadoop3/python/test_support/hello/sub_hello/sub_hello.txt\n","spark-3.5.1-bin-hadoop3/python/test_support/userlib-0.1.zip\n","spark-3.5.1-bin-hadoop3/python/test_support/test_pytorch_training_file.py\n","spark-3.5.1-bin-hadoop3/python/test_support/sql/\n","spark-3.5.1-bin-hadoop3/python/test_support/sql/people_array.json\n","spark-3.5.1-bin-hadoop3/python/test_support/sql/streaming/\n","spark-3.5.1-bin-hadoop3/python/test_support/sql/streaming/text-test.txt\n","spark-3.5.1-bin-hadoop3/python/test_support/sql/orc_partitioned/\n","spark-3.5.1-bin-hadoop3/python/test_support/sql/orc_partitioned/_SUCCESS\n","spark-3.5.1-bin-hadoop3/python/test_support/sql/orc_partitioned/b=0/\n","spark-3.5.1-bin-hadoop3/python/test_support/sql/orc_partitioned/b=0/c=0/\n","spark-3.5.1-bin-hadoop3/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n","spark-3.5.1-bin-hadoop3/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n","spark-3.5.1-bin-hadoop3/python/test_support/sql/orc_partitioned/b=1/\n","spark-3.5.1-bin-hadoop3/python/test_support/sql/orc_partitioned/b=1/c=1/\n","spark-3.5.1-bin-hadoop3/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n","spark-3.5.1-bin-hadoop3/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n","spark-3.5.1-bin-hadoop3/python/test_support/sql/text-test.txt\n","spark-3.5.1-bin-hadoop3/python/test_support/sql/people_array_utf16le.json\n","spark-3.5.1-bin-hadoop3/python/test_support/sql/people1.json\n","spark-3.5.1-bin-hadoop3/python/test_support/sql/ages_newlines.csv\n","spark-3.5.1-bin-hadoop3/python/test_support/sql/people.json\n","spark-3.5.1-bin-hadoop3/python/test_support/SimpleHTTPServer.py\n","spark-3.5.1-bin-hadoop3/python/test_support/userlibrary.py\n","spark-3.5.1-bin-hadoop3/python/pyspark.egg-info/\n","spark-3.5.1-bin-hadoop3/python/pyspark.egg-info/SOURCES.txt\n","spark-3.5.1-bin-hadoop3/python/pyspark.egg-info/top_level.txt\n","spark-3.5.1-bin-hadoop3/python/pyspark.egg-info/PKG-INFO\n","spark-3.5.1-bin-hadoop3/python/pyspark.egg-info/requires.txt\n","spark-3.5.1-bin-hadoop3/python/pyspark.egg-info/dependency_links.txt\n","spark-3.5.1-bin-hadoop3/python/run-tests.py\n","spark-3.5.1-bin-hadoop3/python/setup.cfg\n","spark-3.5.1-bin-hadoop3/python/run-tests\n","spark-3.5.1-bin-hadoop3/python/lib/\n","spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip\n","spark-3.5.1-bin-hadoop3/python/lib/PY4J_LICENSE.txt\n","spark-3.5.1-bin-hadoop3/python/lib/pyspark.zip\n","spark-3.5.1-bin-hadoop3/python/.coveragerc\n","spark-3.5.1-bin-hadoop3/python/docs/\n","spark-3.5.1-bin-hadoop3/python/docs/make.bat\n","spark-3.5.1-bin-hadoop3/python/docs/source/\n","spark-3.5.1-bin-hadoop3/python/docs/source/getting_started/\n","spark-3.5.1-bin-hadoop3/python/docs/source/getting_started/quickstart_connect.ipynb\n","spark-3.5.1-bin-hadoop3/python/docs/source/getting_started/testing_pyspark.ipynb\n","spark-3.5.1-bin-hadoop3/python/docs/source/getting_started/install.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/getting_started/quickstart_ps.ipynb\n","spark-3.5.1-bin-hadoop3/python/docs/source/getting_started/quickstart_df.ipynb\n","spark-3.5.1-bin-hadoop3/python/docs/source/getting_started/index.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/user_guide/\n","spark-3.5.1-bin-hadoop3/python/docs/source/user_guide/arrow_pandas.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/\n","spark-3.5.1-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/types.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/from_to_dbms.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/best_practices.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/typehints.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/faq.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/options.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/pandas_pyspark.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/index.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/transform_apply.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/user_guide/sql/\n","spark-3.5.1-bin-hadoop3/python/docs/source/user_guide/sql/arrow_pandas.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/user_guide/sql/python_udtf.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/user_guide/sql/index.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/user_guide/python_packaging.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/user_guide/index.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/migration_guide/\n","spark-3.5.1-bin-hadoop3/python/docs/source/migration_guide/pyspark_upgrade.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/migration_guide/koalas_to_pyspark.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/migration_guide/index.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/conf.py\n","spark-3.5.1-bin-hadoop3/python/docs/source/_templates/\n","spark-3.5.1-bin-hadoop3/python/docs/source/_templates/version-switcher.html\n","spark-3.5.1-bin-hadoop3/python/docs/source/_templates/autosummary/\n","spark-3.5.1-bin-hadoop3/python/docs/source/_templates/autosummary/class.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/_templates/autosummary/class_with_docs.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/development/\n","spark-3.5.1-bin-hadoop3/python/docs/source/development/debugging.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/development/setting_ide.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/development/errors.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/development/testing.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/development/index.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/development/contributing.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.mllib.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.ss/\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.ss/query_management.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.ss/core_classes.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.ss/io.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.ss/index.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.errors.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/grouping.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/udtf.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/udf.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/observation.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/core_classes.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/io.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/dataframe.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/row.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/catalog.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/avro.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/protobuf.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/configuration.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/spark_session.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/window.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/column.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/functions.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/index.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/data_types.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.streaming.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.resource.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.testing.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/ml.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/general_functions.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/indexing.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/extensions.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/io.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/series.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/frame.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/groupby.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/resampling.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/window.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/index.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/index.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/reference/pyspark.ml.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/index.rst\n","spark-3.5.1-bin-hadoop3/python/docs/source/_static/\n","spark-3.5.1-bin-hadoop3/python/docs/source/_static/css/\n","spark-3.5.1-bin-hadoop3/python/docs/source/_static/css/pyspark.css\n","spark-3.5.1-bin-hadoop3/python/docs/source/_static/versions.json\n","spark-3.5.1-bin-hadoop3/python/docs/Makefile\n","spark-3.5.1-bin-hadoop3/python/docs/make2.bat\n","spark-3.5.1-bin-hadoop3/python/.gitignore\n","spark-3.5.1-bin-hadoop3/python/dist/\n","spark-3.5.1-bin-hadoop3/python/mypy.ini\n","spark-3.5.1-bin-hadoop3/python/README.md\n","spark-3.5.1-bin-hadoop3/python/run-tests-with-coverage\n","spark-3.5.1-bin-hadoop3/python/pyspark/\n","spark-3.5.1-bin-hadoop3/python/pyspark/resultiterable.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/status.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/statcounter.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/rdd.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/streaming/\n","spark-3.5.1-bin-hadoop3/python/pyspark/streaming/kinesis.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/streaming/context.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/streaming/util.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/streaming/tests/\n","spark-3.5.1-bin-hadoop3/python/pyspark/streaming/tests/test_context.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/streaming/tests/test_listener.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/streaming/tests/test_dstream.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/streaming/tests/test_kinesis.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/streaming/tests/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/streaming/listener.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/streaming/dstream.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/streaming/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/resource/\n","spark-3.5.1-bin-hadoop3/python/pyspark/resource/requests.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/resource/profile.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/resource/tests/\n","spark-3.5.1-bin-hadoop3/python/pyspark/resource/tests/test_resources.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/resource/tests/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/resource/information.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/resource/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/conf.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/join.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/dl_util.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/pipeline.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/recommendation.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/model_cache.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/connect/\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/connect/pipeline.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/connect/functions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/connect/util.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/connect/tuning.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/connect/summarizer.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/connect/evaluation.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/connect/base.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/connect/classification.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/connect/feature.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/connect/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/connect/io_utils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/regression.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/functions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/wrapper.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/common.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/util.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/clustering.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/param/\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/param/_shared_params_code_gen.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/param/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/param/shared.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/_typing.pyi\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/test_base.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/test_training_summary.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/connect/\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/connect/test_connect_classification.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/connect/test_legacy_mode_classification.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/connect/test_connect_tuning.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/connect/test_legacy_mode_tuning.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/connect/test_connect_summarizer.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/connect/test_connect_evaluation.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/connect/test_parity_torch_distributor.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/connect/test_legacy_mode_evaluation.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/connect/test_legacy_mode_pipeline.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/connect/test_legacy_mode_feature.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/connect/test_legacy_mode_summarizer.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/connect/test_connect_function.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/connect/test_connect_feature.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/connect/test_connect_pipeline.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/connect/test_parity_torch_data_loader.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/test_persistence.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/test_pipeline.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/test_dl_util.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/test_functions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/test_algorithms.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/test_image.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/typing/\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/typing/test_evaluation.yml\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/typing/test_param.yml\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/typing/test_readable.yml\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/typing/test_clustering.yaml\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/typing/test_regression.yml\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/typing/test_feature.yml\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/typing/test_classification.yml\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/test_feature.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/test_util.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/tuning/\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/tuning/test_tvs_io_nested.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/tuning/test_cv_io_basic.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/tuning/test_tuning.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/tuning/test_cv_io_nested.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/tuning/test_tvs_io_basic.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/tuning/test_tvs_io_pipeline.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/tuning/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/tuning/test_cv_io_pipeline.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/test_linalg.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/test_evaluation.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/test_stat.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/test_param.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/test_wrapper.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tests/test_model_cache.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tuning.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/linalg/\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/linalg/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/image.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/evaluation.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/torch/\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/torch/tests/\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/torch/tests/test_data_loader.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/torch/tests/test_log_communication.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/torch/tests/test_distributor.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/torch/tests/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/torch/data.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/torch/distributor.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/torch/torch_run_process_wrapper.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/torch/log_communication.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/torch/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/base.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/classification.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/deepspeed/\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/deepspeed/tests/\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/deepspeed/tests/test_deepspeed_distributor.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/deepspeed/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/deepspeed/deepspeed_distributor.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/stat.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/feature.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/fpm.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/ml/tree.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/sql_formatter.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/proto/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/proto/expressions_pb2.pyi\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/proto/commands_pb2.pyi\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/proto/example_plugins_pb2.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/proto/types_pb2.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/proto/base_pb2.pyi\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/proto/base_pb2.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/proto/example_plugins_pb2.pyi\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/proto/commands_pb2.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/proto/common_pb2.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/proto/relations_pb2.pyi\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/proto/relations_pb2.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/proto/base_pb2_grpc.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/proto/expressions_pb2.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/proto/common_pb2.pyi\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/proto/catalog_pb2.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/proto/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/proto/types_pb2.pyi\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/proto/catalog_pb2.pyi\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/column.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/streaming/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/streaming/query.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/streaming/readwriter.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/streaming/worker/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/streaming/worker/listener_worker.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/streaming/worker/foreach_batch_worker.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/streaming/worker/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/streaming/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/functions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/conf.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/udf.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/plan.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/client/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/client/artifact.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/client/reattach.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/client/core.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/client/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/conversion.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/session.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/group.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/udtf.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/_typing.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/avro/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/avro/functions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/avro/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/protobuf/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/protobuf/functions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/protobuf/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/window.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/catalog.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/utils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/readwriter.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/expressions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/dataframe.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/types.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/connect/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/column.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/streaming/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/streaming/query.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/streaming/state.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/streaming/listener.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/streaming/readwriter.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/streaming/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/functions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/conf.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/context.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/observation.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/udf.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/_typing.pyi\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/streaming/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/streaming/test_parity_streaming.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/streaming/test_parity_foreach_batch.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/streaming/test_parity_foreach.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/streaming/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/streaming/test_parity_listener.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_pandas_grouped_map_with_state.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_conf.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_datasources.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_arrow_map.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_utils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_catalog.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_session.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_pandas_udf_grouped_agg.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/client/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/client/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/client/test_client.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/client/test_artifact.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_serde.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_connect_basic.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_connect_plan.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_types.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_arrow.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_pandas_udf.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_arrow_python_udf.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_readwriter.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_pandas_udf_scalar.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_udf.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_pandas_grouped_map.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_connect_column.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_pandas_cogrouped_map.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_errors.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_column.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_pandas_udf_window.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_functions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_connect_function.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_udtf.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_dataframe.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_group.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_pandas_map.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_readwriter.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_serde.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_arrow_map.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_pandas_sqlmetrics.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_udtf.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/streaming/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/streaming/test_streaming_listener.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/streaming/test_streaming_foreach_batch.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/streaming/test_streaming.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/streaming/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/streaming/test_streaming_foreach.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_group.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_context.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_functions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_datasources.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_catalog.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_utils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_errors.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_session.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_arrow.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_dataframe.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/typing/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/typing/test_column.yml\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/typing/test_dataframe.yml\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/typing/test_session.yml\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/typing/test_functions.yml\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/typing/test_udf.yml\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/typing/test_readwriter.yml\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/pandas/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_cogrouped_map.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_grouped_map_with_state.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_udf_grouped_agg.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_grouped_map.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_udf_scalar.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_udf_typehints.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_udf_typehints_with_future_annotations.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_udf.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_udf_window.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/pandas/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_map.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_types.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_arrow_python_udf.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_column.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_udf.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_udf_profiler.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/tests/test_conf.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/session.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/group.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/udtf.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/pandas/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/pandas/group_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/pandas/functions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/pandas/map_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/pandas/conversion.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/pandas/typehints.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/pandas/_typing/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/pandas/_typing/protocols/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/pandas/_typing/protocols/series.pyi\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/pandas/_typing/protocols/frame.pyi\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/pandas/_typing/protocols/__init__.pyi\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/pandas/_typing/__init__.pyi\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/pandas/functions.pyi\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/pandas/serializers.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/pandas/utils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/pandas/types.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/pandas/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/avro/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/avro/functions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/avro/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/protobuf/\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/protobuf/functions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/protobuf/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/window.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/catalog.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/utils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/readwriter.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/dataframe.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/types.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/sql/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/context.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/util.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/accumulators.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/rddsampler.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/_typing.pyi\n","spark-3.5.1-bin-hadoop3/python/pyspark/py.typed\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_rddbarrier.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_context.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_profiler.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_broadcast.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_pin_thread.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_rdd.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_readwrite.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/typing/\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/typing/test_resultiterable.yml\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/typing/test_context.yml\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/typing/test_rdd.yml\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/typing/test_core.yml\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_rddsampler.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_util.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_appsubmit.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_daemon.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_shuffle.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_taskcontext.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_serializers.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_worker.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_memory_profiler.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_install_spark.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_conf.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_statcounter.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_stage_sched.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/tests/test_join.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/storagelevel.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/profiler.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/version.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/instrumentation_utils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/worker_util.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/worker.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/cloudpickle/\n","spark-3.5.1-bin-hadoop3/python/pyspark/cloudpickle/compat.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/cloudpickle/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/testing/\n","spark-3.5.1-bin-hadoop3/python/pyspark/testing/pandasutils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/testing/mlutils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/testing/sqlutils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/testing/streamingutils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/testing/connectutils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/testing/utils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/testing/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/testing/mllibutils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/__pycache__/\n","spark-3.5.1-bin-hadoop3/python/pyspark/__pycache__/install.cpython-38.pyc\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/frame.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/sql_formatter.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/categorical.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/namespace.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/accessors.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/usage_logging/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/usage_logging/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/usage_logging/usage_logger.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/correlation.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/indexing.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/groupby.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/resample.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/plot/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/plot/plotly.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/plot/core.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/plot/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/plot/matplotlib.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/supported_api_gen.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/null_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/num_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/udt_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/string_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/complex_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/date_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/categorical_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/timedelta_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/datetime_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/base.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/binary_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/boolean_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_rolling.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_sql.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_dataframe_spark_io.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_ops_on_diff_frames_groupby.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/computation/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/computation/test_parity_apply_func.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/computation/test_parity_compute.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/computation/test_parity_any_all.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/computation/test_parity_melt.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/computation/test_parity_pivot.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/computation/test_parity_cov.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/computation/test_parity_describe.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/computation/test_parity_corrwith.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/computation/test_parity_combine.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/computation/test_parity_cumulative.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/computation/test_parity_eval.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/computation/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/computation/test_parity_missing_data.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/computation/test_parity_binary_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_utils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/groupby/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/groupby/test_parity_apply_func.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/groupby/test_parity_split_apply.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/groupby/test_parity_stat.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/groupby/test_parity_index.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/groupby/test_parity_describe.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/groupby/test_parity_cumulative.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/groupby/test_parity_aggregate.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/groupby/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/groupby/test_parity_head_tail.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/groupby/test_parity_missing_data.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/groupby/test_parity_groupby.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/diff_frames_ops/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/diff_frames_ops/test_parity_index.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/diff_frames_ops/test_parity_basic_slow.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/diff_frames_ops/test_parity_setitem_frame.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/diff_frames_ops/test_parity_series.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/diff_frames_ops/test_parity_cov_corrwith.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/diff_frames_ops/test_parity_align.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/diff_frames_ops/test_parity_dot_frame.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/diff_frames_ops/test_parity_dot_series.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/diff_frames_ops/test_parity_setitem_series.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/diff_frames_ops/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/frame/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/frame/test_parity_attrs.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/frame/test_parity_conversion.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/frame/test_parity_constructor.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/frame/test_parity_reindexing.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/frame/test_parity_truncate.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/frame/test_parity_take.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/frame/test_parity_spark.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/frame/test_parity_reshaping.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/frame/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/frame/test_parity_time_series.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_extension.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_ops_on_diff_frames.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/io/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/io/test_parity_io.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/io/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/plot/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/plot/test_parity_frame_plot_plotly.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/plot/test_parity_series_plot_plotly.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/plot/test_parity_frame_plot_matplotlib.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/plot/test_parity_series_plot_matplotlib.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/plot/test_parity_frame_plot.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/plot/test_parity_series_plot.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/plot/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_indexops_spark.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_typedef.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_spark_functions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_stats.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_dataframe_conversion.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_default_index.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/data_type_ops/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/data_type_ops/test_parity_date_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/data_type_ops/test_parity_null_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/data_type_ops/test_parity_string_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/data_type_ops/test_parity_categorical_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/data_type_ops/test_parity_base.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/data_type_ops/test_parity_timedelta_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/data_type_ops/test_parity_complex_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/data_type_ops/test_parity_datetime_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/data_type_ops/test_parity_num_arithmetic.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/data_type_ops/test_parity_num_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/data_type_ops/test_parity_boolean_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/data_type_ops/test_parity_udt_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/data_type_ops/testing_utils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/data_type_ops/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/data_type_ops/test_parity_binary_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/data_type_ops/test_parity_num_reverse.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_ops_on_diff_frames_groupby_expanding.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/indexes/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/indexes/test_parity_category.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/indexes/test_parity_timedelta.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/indexes/test_parity_base.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/indexes/test_parity_reindex.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/indexes/test_parity_reset_index.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/indexes/test_parity_align.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/indexes/test_parity_indexing.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/indexes/test_parity_datetime.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/indexes/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/indexes/test_parity_rename.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_series_string.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_generic_functions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_ops_on_diff_frames_groupby_rolling.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_series_conversion.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_numpy_compat.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/series/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/series/test_parity_stat.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/series/test_parity_compute.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/series/test_parity_sort.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/series/test_parity_conversion.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/series/test_parity_all_any.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/series/test_parity_index.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/series/test_parity_series.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/series/test_parity_cumulative.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/series/test_parity_as_type.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/series/test_parity_arg_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/series/test_parity_as_of.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/series/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/series/test_parity_missing_data.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_series_datetime.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_ewm.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_window.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_config.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_indexing.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_internal.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_reshape.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_resample.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_frame_spark.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_categorical.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_rolling.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_scalars.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_expanding.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_repr.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_csv.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/connect/test_parity_namespace.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/computation/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/computation/test_combine.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/computation/test_pivot.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/computation/test_any_all.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/computation/test_melt.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/computation/test_apply_func.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/computation/test_eval.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/computation/test_corrwith.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/computation/test_cov.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/computation/test_cumulative.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/computation/test_missing_data.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/computation/test_describe.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/computation/test_compute.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/computation/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/computation/test_binary_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_series_string.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_typedef.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_ewm.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/groupby/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/groupby/test_index.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/groupby/test_apply_func.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/groupby/test_groupby.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/groupby/test_head_tail.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/groupby/test_cumulative.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/groupby/test_split_apply.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/groupby/test_missing_data.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/groupby/test_stat.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/groupby/test_describe.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/groupby/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/groupby/test_aggregate.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/diff_frames_ops/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/diff_frames_ops/test_dot_frame.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/diff_frames_ops/test_dot_series.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/diff_frames_ops/test_align.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/diff_frames_ops/test_index.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/diff_frames_ops/test_setitem_frame.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/diff_frames_ops/test_series.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/diff_frames_ops/test_cov_corrwith.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/diff_frames_ops/test_setitem_series.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/diff_frames_ops/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/diff_frames_ops/test_basic_slow.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_dataframe_conversion.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/frame/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/frame/test_time_series.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/frame/test_take.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/frame/test_constructor.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/frame/test_reshaping.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/frame/test_attrs.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/frame/test_reindexing.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/frame/test_truncate.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/frame/test_conversion.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/frame/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/frame/test_spark.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_numpy_compat.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_scalars.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_sql.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_frame_spark.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/io/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/io/test_io.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/io/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_utils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_extension.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/plot/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/plot/test_series_plot.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/plot/test_frame_plot_matplotlib.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/plot/test_series_plot_plotly.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/plot/test_series_plot_matplotlib.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/plot/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/plot/test_frame_plot_plotly.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/plot/test_frame_plot.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_base.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_datetime_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_timedelta_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_num_reverse.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_complex_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_boolean_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_num_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/testing_utils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_null_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_num_arithmetic.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_string_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_categorical_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_binary_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_udt_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_date_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_spark_functions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_resample.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby_expanding.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_namespace.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/indexes/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_base.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_align.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_reset_index.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_rename.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_reindex.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_category.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/indexes/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_indexing.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_timedelta.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_datetime.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_expanding.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/series/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/series/test_index.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/series/test_arg_ops.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/series/test_as_type.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/series/test_series.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/series/test_cumulative.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/series/test_all_any.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/series/test_as_of.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/series/test_missing_data.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/series/test_stat.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/series/test_conversion.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/series/test_compute.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/series/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/series/test_sort.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_default_index.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_repr.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_dataframe_spark_io.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_internal.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_series_datetime.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_generic_functions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_reshape.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_csv.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_window.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_stats.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_config.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby_rolling.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_series_conversion.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_indexing.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_indexops_spark.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/tests/test_categorical.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/internal.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/indexes/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/indexes/timedelta.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/indexes/datetimes.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/indexes/category.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/indexes/base.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/indexes/numeric.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/indexes/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/indexes/multi.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/numpy_compat.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/datetimes.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/missing/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/missing/frame.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/missing/groupby.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/missing/common.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/missing/resample.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/missing/indexes.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/missing/general_functions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/missing/series.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/missing/window.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/missing/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/missing/scalars.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/_typing.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/spark/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/spark/accessors.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/spark/functions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/spark/utils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/spark/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/series.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/base.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/window.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/utils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/exceptions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/mlflow.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/extensions.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/generic.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/config.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/strings.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/typedef/\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/typedef/typehints.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/typedef/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/pandas/sql_processor.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/recommendation.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/regression.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/common.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/util.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/clustering.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/_typing.pyi\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/tests/\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/tests/test_streaming_algorithms.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/tests/test_algorithms.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/tests/test_feature.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/tests/test_util.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/tests/test_linalg.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/tests/test_stat.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/tests/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/linalg/\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/linalg/distributed.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/linalg/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/evaluation.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/stat/\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/stat/distribution.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/stat/KernelDensity.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/stat/test.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/stat/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/stat/_statistics.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/classification.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/feature.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/fpm.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/tree.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/mllib/random.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/broadcast.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/daemon.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/traceback_utils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/serializers.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/errors/\n","spark-3.5.1-bin-hadoop3/python/pyspark/errors/tests/\n","spark-3.5.1-bin-hadoop3/python/pyspark/errors/tests/test_errors.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/errors/tests/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/errors/error_classes.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/errors/utils.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/errors/exceptions/\n","spark-3.5.1-bin-hadoop3/python/pyspark/errors/exceptions/connect.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/errors/exceptions/base.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/errors/exceptions/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/errors/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/shell.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/python/\n","spark-3.5.1-bin-hadoop3/python/pyspark/python/pyspark/\n","spark-3.5.1-bin-hadoop3/python/pyspark/python/pyspark/shell.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/taskcontext.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/files.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/__init__.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/install.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/_globals.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/shuffle.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/java_gateway.py\n","spark-3.5.1-bin-hadoop3/python/pyspark/find_spark_home.py\n","spark-3.5.1-bin-hadoop3/RELEASE\n","spark-3.5.1-bin-hadoop3/R/\n","spark-3.5.1-bin-hadoop3/R/lib/\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/Meta/\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/Meta/nsInfo.rds\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/Meta/hsearch.rds\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/Meta/vignette.rds\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/Meta/Rd.rds\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/Meta/package.rds\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/Meta/links.rds\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/Meta/features.rds\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/doc/\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/doc/index.html\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/doc/sparkr-vignettes.html\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/doc/sparkr-vignettes.Rmd\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/doc/sparkr-vignettes.R\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/help/\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/help/SparkR.rdx\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/help/paths.rds\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/help/SparkR.rdb\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/help/AnIndex\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/help/aliases.rds\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/NAMESPACE\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/tests/\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/tests/testthat/\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/tests/testthat/test_basic.R\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/INDEX\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/DESCRIPTION\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/profile/\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/profile/general.R\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/profile/shell.R\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/worker/\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/worker/daemon.R\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/worker/worker.R\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/html/\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/html/00Index.html\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/html/R.css\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/R/\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/R/SparkR.rdx\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/R/SparkR.rdb\n","spark-3.5.1-bin-hadoop3/R/lib/SparkR/R/SparkR\n","spark-3.5.1-bin-hadoop3/R/lib/sparkr.zip\n"]}]},{"cell_type":"code","source":["from google.colab import drive, files\n","from pyspark.sql import SparkSession\n","import findspark\n","import requests\n","import gzip\n","import tarfile\n","import os\n","from pyspark.sql.types import StructType, StructField, StringType\n","from pyspark.sql import Row\n","from pyspark.sql.functions import col, count, sum, avg, when\n","findspark.init()\n","\n"],"metadata":{"id":"Pf28b6x9Qq_c","executionInfo":{"status":"ok","timestamp":1750531433898,"user_tz":180,"elapsed":583,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["spark = SparkSession.builder \\\n","    .appName(\"IFood Test Case\") \\\n","    .getOrCreate()\n","\n","spark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"A2Nr583tQw6n","outputId":"d317ab8d-fa7f-48ca-b79b-bd6d18560725","executionInfo":{"status":"ok","timestamp":1750531446921,"user_tz":180,"elapsed":12624,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}}},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7c496cc47b10>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://713ab9e3165d:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.5.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>IFood Test Case</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["###################################################################################\n","# SEÇÃO DE CONFIGURAÇÃO: ORIGEM DOS DADOS\n","# ---------------------------------------------------------------------------------\n","# Define se os arquivos de dados serão lidos a partir do Google Drive ou de uma\n","# pasta local (uploaded).\n","###################################################################################\n","\n","# Variável de controle para a fonte dos dados.\n","# Opções válidas:\n","#   \"drive\": Para carregar os arquivos diretamente de uma pasta no Google Drive.\n","#   \"uploaded\": Para carregar os arquivos de um diretório local.\n","data_source = \"drive\"\n","\n","# Define o caminho base no Google Drive onde os arquivos de dados estão localizados.\n","# Esta variável só é utilizada se 'data_source' for definida como \"drive\".\n","# Exemplo de estrutura de pastas: drive/MyDrive/iFood/Data/Bronze/\n","drive_silver_path = \"drive/MyDrive/iFood/Data/Silver\"\n","upload_silver_path = \"./Data/Silver\"  ##Pode variar caso os .parquets nao sejam exatamente os criados no primeiro notebook.\n","\n","if data_source == \"drive\":\n","    drive.mount('/content/drive')\n","    silver_path = drive_silver_path\n","else:\n","    silver_path = upload_silver_path\n"],"metadata":{"id":"48Tlf20Al4zN","executionInfo":{"status":"ok","timestamp":1750531464767,"user_tz":180,"elapsed":17834,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cc18540c-5776-4784-b578-aa4377fd39df"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["### apenas para o fluxo usando upload manual...###########\n","######################################################\n","\n","# Criar diretório local para salvar os arquivos\n","os.makedirs('Data', exist_ok=True)\n","# depois faca o upload dos zip nessa pasta, caso nao esteja usando o fluxo do google drive.\n"],"metadata":{"id":"xvMukfrlfjTn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### apenas para o fluxo usando upload manual...###########\n","######################################################\n","\n","# Define uma variável para armazenar o nome da pasta onde os arquivos .zip\n","# devem ser encontrados. O caminho \"./Data\" refere-se a uma pasta chamada \"Data\"\n","# localizada no mesmo nível do diretório de execução do notebook (/content/).\n","caminho_da_pasta_zip = \"./Data\"\n","\n","\n","# Imprime uma mensagem informativa para o usuário saber qual ação está sendo executada.\n","print(f\"\\nProcurando arquivos .zip em: {caminho_da_pasta_zip}\")\n","\n","# Bloco de verificação para garantir que a pasta de origem existe.\n","# Se a pasta não for encontrada, o script exibe um erro claro em vez de falhar.\n","if not os.path.isdir(caminho_da_pasta_zip):\n","    print(f\"ERRO: O diretório '{caminho_da_pasta_zip}' não foi encontrado.\")\n","    print(\"Por favor, verifique o caminho e tente novamente.\")\n","else:\n","    # Se a pasta existe, esta linha obtém uma lista de todos os nomes de arquivos e subpastas dentro dela.\n","    arquivos_na_pasta = os.listdir(caminho_da_pasta_zip)\n","\n","    # Inicializa uma variável de controle para rastrear se algum arquivo .zip foi processado.\n","    zip_encontrado = False\n","\n","    # Inicia um loop para examinar cada item encontrado na pasta de origem.\n","    for nome_arquivo in arquivos_na_pasta:\n","        # Condição para filtrar e processar apenas os itens que são arquivos .zip.\n","        if nome_arquivo.endswith(\".zip\"):\n","            # Atualiza a variável de controle, indicando que pelo menos um zip foi encontrado.\n","            zip_encontrado = True\n","            # Constrói o caminho completo para o arquivo .zip, unindo o nome da pasta e o nome do arquivo.\n","            caminho_completo_zip = os.path.join(caminho_da_pasta_zip, nome_arquivo)\n","\n","            print(f\"  -> Descompactando '{nome_arquivo}'...\")\n","\n","            # Utiliza um comando de shell (!unzip) para extrair o conteúdo do arquivo.\n","            # A opção \"-o\" força a substituição de arquivos existentes sem pedir permissão.\n","            # A opção \"-d /content/\" especifica o diretório de destino para a extração.\n","            !unzip -o \"{caminho_completo_zip}\" -d /content/\n","\n","    # Após o loop, se a variável de controle não mudou, significa que nenhum zip foi processado.\n","    if not zip_encontrado:\n","        print(\"Nenhum arquivo .zip foi encontrado na pasta especificada.\")\n","\n","# Mensagem final para indicar que todas as operações foram concluídas.\n","print(\"\\nProcesso de descompactação concluído! ✅\")\n","\n","\n","# Bloco final para verificação visual dos resultados.\n","# O comando de shell \"!ls\" lista o conteúdo do diretório especificado.\n","print(\"\\nConteúdo extraído no diretório (/content/Data/Bronze/):\")\n","!ls -l /content/Data/Bronze"],"metadata":{"id":"-0TFRYHJfkMb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Caminhos para os dados de pedidos\n","orders_silver_path = f\"{silver_path}/orders.parquet\"\n","\n","# Caminhos para os dados de consumidores\n","consumers_silver_path = f\"{silver_path}/consumers.parquet\"\n","\n","# Caminhos para os dados de restaurantes\n","restaurants_silver_path = f\"{silver_path}/restaurants.parquet\"\n","\n","# Caminhos para os dados de referencia do teste A/B\n","ab_test_silver_path = f\"{silver_path}/ab_test_ref.parquet\"\n"],"metadata":{"id":"ewhKG2ZteCY2","executionInfo":{"status":"ok","timestamp":1750531467699,"user_tz":180,"elapsed":3,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["dfp_orders = spark.read.parquet(orders_silver_path)\n","dfp_consumers = spark.read.parquet(consumers_silver_path)\n","dfp_restaurants = spark.read.parquet(restaurants_silver_path)\n","dfp_ab_test = spark.read.parquet(ab_test_silver_path)"],"metadata":{"id":"vItuW2GBf55X","executionInfo":{"status":"ok","timestamp":1750531480269,"user_tz":180,"elapsed":11909,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# --- Bloco de Verificação: Validação da Carga de Dados na Camada Silver ---\n","#\n","# O objetivo deste bloco é realizar um teste de sanidade (sanity check) para\n","# garantir que a ingestão de dados para a camada Silver ocorreu como esperado.\n","#\n","\n","# Contagem de registros no DataFrame de pedidos (orders).\n","print(\"Contagem de registros em 'dfp_orders':\")\n","print(dfp_orders.count())\n","\n","# Contagem de registros no DataFrame de consumidores (consumers).\n","print(\"\\nContagem de registros em 'dfp_consumers':\")\n","print(dfp_consumers.count())\n","\n","# Contagem de registros no DataFrame de restaurantes (restaurants).\n","print(\"\\nContagem de registros em 'dfp_restaurants':\")\n","print(dfp_restaurants.count())\n","\n","# Contagem de registros no DataFrame de teste A/B (ab_test).\n","print(\"\\nContagem de registros em 'dfp_ab_test':\")\n","print(dfp_ab_test.count())"],"metadata":{"id":"i9xICtx8qveD","executionInfo":{"status":"ok","timestamp":1750432858186,"user_tz":180,"elapsed":15480,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ce1a97d1-370c-426e-c4ce-2029dccfbe67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Contagem de registros em 'dfp_orders':\n","2426590\n","\n","Contagem de registros em 'dfp_consumers':\n","804559\n","\n","Contagem de registros em 'dfp_restaurants':\n","7292\n","\n","Contagem de registros em 'dfp_ab_test':\n","804559\n"]}]},{"cell_type":"code","source":["print(\"--- Schema da Tabela: orders ---\")\n","# Exibe a estrutura (colunas, tipos e nulidade) da tabela de pedidos.\n","dfp_orders.printSchema()\n","\n","print(\"\\n--- Schema da Tabela: consumers ---\")\n","# Exibe a estrutura da tabela de clientes.\n","dfp_consumers.printSchema()\n","\n","print(\"\\n--- Schema da Tabela: restaurants ---\")\n","# Exibe a estrutura da tabela de restaurantes.\n","dfp_restaurants.printSchema()\n","\n","print(\"\\n--- Schema da Tabela: ab_test ---\")\n","# Exibe a estrutura da tabela de referência do teste A/B.\n","dfp_ab_test.printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L1pKswXng4iG","executionInfo":{"status":"ok","timestamp":1750376524022,"user_tz":180,"elapsed":15,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"a3410db7-2268-4235-de07-4c3739bb34d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Schema da Tabela: orders ---\n","root\n"," |-- order_id: string (nullable = true)\n"," |-- customer_id: string (nullable = true)\n"," |-- merchant_id: string (nullable = true)\n"," |-- order_created_at: string (nullable = true)\n"," |-- order_total_amount: double (nullable = true)\n"," |-- delivery_address_city: string (nullable = true)\n"," |-- delivery_address_state: string (nullable = true)\n"," |-- delivery_address_latitude: string (nullable = true)\n"," |-- delivery_address_longitude: string (nullable = true)\n"," |-- origin_platform: string (nullable = true)\n"," |-- order_category: string (nullable = true)\n","\n","\n","--- Schema da Tabela: consumers ---\n","root\n"," |-- customer_id: string (nullable = true)\n"," |-- created_at: timestamp (nullable = true)\n"," |-- customer_phone_state: string (nullable = true)\n","\n","\n","--- Schema da Tabela: restaurants ---\n","root\n"," |-- id: string (nullable = true)\n"," |-- created_at: timestamp (nullable = true)\n"," |-- price_range: integer (nullable = true)\n"," |-- average_ticket: double (nullable = true)\n"," |-- delivery_time: integer (nullable = true)\n"," |-- minimum_order_value: double (nullable = true)\n"," |-- merchant_city: string (nullable = true)\n"," |-- merchant_state: string (nullable = true)\n","\n","\n","--- Schema da Tabela: ab_test ---\n","root\n"," |-- customer_id: string (nullable = true)\n"," |-- is_target: string (nullable = true)\n"," |-- created_at: timestamp (nullable = true)\n"," |-- customer_phone_state: string (nullable = true)\n","\n"]}]},{"cell_type":"markdown","source":["###DESAFIO 2####\n","A criação de segmentações permite agrupar usuários de acordo com\n","características e comportamentos similares, possibilitando criar estraté\n","gias direcionadas de acordo com o perfil de cada público, facilitando a\n","personalização e incentivando o engajamento, retenção, além de otimi\n","zação de recursos. Segmentações de usuários são muito utilizadas pe\n","los times de Data, mas a área em que você atua ainda não tem seg\n","mentos bem definidos e cada área de Negócio utiliza conceitos dife\n","rentes. Por isso, você precisa:\n","\n","- a) Definir as segmentações que fazem sentido especificamente\n","para o teste A/B que está analisando.\n","\n","- b) Estabelecer quais serão os critérios utilizados para cada seg\n","mento sugerido no item a). Utilize os critérios/ferramentas que\n","achar necessários, mas lembre-se de explicar o racional utiliza\n","do na criação.\n","\n","- c) Analisar os resultados do teste A/B com base nos segmentos\n","definidos nos itens a) e b)."],"metadata":{"id":"YzyEjrJ_gX3L"}},{"cell_type":"code","source":["from pyspark.sql.functions import to_date, col\n","\n","# Converte a coluna 'created_at' para o tipo Date, caso ainda não seja.\n","# Assumimos que 'created_at' já foi limpa e está em um formato reconhecível (e.g., 'yyyy-MM-dd HH:mm:ss')\n","# Se a conversão para Parquet já tratou isso, esta linha pode ser redundante, mas é uma boa prática garantir.\n","dfp_consumers = dfp_consumers.withColumn(\"created_at_date\", to_date(col(\"created_at\")))\n","\n","# Define a data de corte\n","cut_off_date = \"2018-12-01\"\n","\n","# Filtra os consumidores criados antes de dezembro de 2018\n","consumers_before_dec_2018 = dfp_consumers.filter(col(\"created_at_date\") < cut_off_date)\n","\n","# Filtra os consumidores criados a partir de dezembro de 2018\n","consumers_from_dec_2018 = dfp_consumers.filter(col(\"created_at_date\") >= cut_off_date)\n","\n","# Conta o número de consumidores em cada grupo\n","count_before = consumers_before_dec_2018.count()\n","count_from = consumers_from_dec_2018.count()\n","\n","# Imprime os resultados\n","print(f\"Número de consumidores criados antes de Dezembro de 2018: {count_before}\")\n","print(f\"Número de consumidores criados a partir de Dezembro de 2018: {count_from}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2_TkWK5elDW_","executionInfo":{"status":"ok","timestamp":1750376733133,"user_tz":180,"elapsed":4668,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"e3c95d48-f2d4-419b-dbc8-3216c1143c4f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Número de consumidores criados antes de Dezembro de 2018: 804559\n","Número de consumidores criados a partir de Dezembro de 2018: 0\n"]}]},{"cell_type":"markdown","source":["# Proposta de criar segmentação por tempo de vida é descartada pois para base que temos os valores sao homogeneos demais para gerar algum valor de analise."],"metadata":{"id":"0CtigTvknY8Y"}},{"cell_type":"markdown","source":["## Desafio 2: Estratégia de Segmentação de Usuários\n","\n","Com o impacto geral da campanha validado, o próximo passo é aprofundar a análise através da criação de segmentos de usuários. O objetivo é sair de uma abordagem de \"um tamanho para todos\" e entender como diferentes perfis de clientes reagiram à campanha de cupons. Isso nos permitirá otimizar futuras ações, tornando-as mais direcionadas, eficientes e lucrativas.\n","\n","### O Framework de Segmentação Proposto\n","\n","Para obter uma visão completa do comportamento do usuário, propomos uma abordagem de segmentação em duas camadas, que podem ser usadas em conjunto ou separadamente para criar \"personas\" detalhadas:\n","\n","1.  **Segmentação por Padrão de Consumo (Dias da Semana):** Responde à pergunta \"Qual o papel do iFood na vida do cliente?\".\n","2.  **Segmentação por Valor e Engajamento (Modelo RFM):** Responde à pergunta \"Qual o valor deste cliente para o negócio?\".\n","\n","A combinação das duas nos dará uma visão 360º de cada perfil de usuário.\n","\n","### a) e b): Definição dos Segmentos e Seus Critérios\n","\n","A seguir, detalhamos cada segmento proposto e os critérios exatos para sua criação, respondendo formalmente aos itens **2a e 2b**.\n","\n","#### 2.1. Segmentação por Padrão de Consumo (Dias da Semana)\n","\n","* **Racional:** Esta segmentação busca entender se o iFood é uma solução para a conveniência do dia a dia (almoço no trabalho, jantar durante a semana) ou uma ferramenta para momentos de lazer e socialização (pedidos de fim de semana). Cada um desses contextos apresenta oportunidades de marketing distintas.\n","\n","* **Critérios (a serem calculados a partir dos dados de pedidos):**\n","    1.  Para cada pedido, extraímos o dia da semana.\n","    2.  Para cada cliente, calculamos a porcentagem de seus pedidos que ocorreram em \"dias de semana\" (Segunda a Quinta) vs. \"fim de semana\" (Sexta a Domingo).\n","    3.  Com base nessa proporção, criamos os seguintes segmentos:\n","\n","| Segmento | Critério Sugerido | Perfil do Cliente e Comportamento |\n","| :--- | :--- | :--- |\n","| **Usuário de Fim de Semana** | > 70% dos pedidos ocorrem de Sexta a Domingo. | Vê o iFood como uma recompensa ou uma solução para encontros sociais. Provavelmente pede para mais de uma pessoa. |\n","| **Usuário de Dias de Semana** | > 70% dos pedidos ocorrem de Segunda a Quinta. | Vê o iFood como uma solução prática e funcional para a rotina. Pode ser mais sensível a preço e velocidade de entrega. |\n","| **Usuário Híbrido/Intensivo**| Pedidos distribuídos de forma equilibrada pela semana. | Perfil mais engajado. O iFood faz parte de toda a sua rotina, tanto para conveniência quanto para lazer. Provavelmente são os clientes de maior valor. |\n","\n","#### 2.2. Segmentação por Valor e Engajamento (Modelo RFM)\n","\n","* **Racional:** Este é o modelo padrão da indústria para quantificar o comportamento de compra e a \"saúde\" da conta de um cliente. Ele é altamente preditivo: clientes que compraram recentemente, com frequência e gastaram mais, têm a maior probabilidade de comprar novamente. O racional é focar os esforços de marketing de forma diferente em cada estrato de valor.\n","\n","* **Critérios (calculados para cada cliente):**\n","    * **Recência (R):** Há quanto tempo o cliente fez seu último pedido. (Menor = Melhor)\n","    * **Frequência (F):** Com que frequência o cliente compra. (Maior = Melhor)\n","    * **Valor Monetário (M):** Quanto dinheiro o cliente gasta no total. (Maior = Melhor)\n","\n","* **Criação dos Segmentos (Método de Pontuação):**\n","    1.  Calculamos R, F e M para cada cliente.\n","    2.  Dividimos os clientes em quartis (4 grupos) para cada métrica e atribuímos uma pontuação de 1 a 4 (onde 4 é o melhor comportamento).\n","    3.  Combinando as pontuações, criamos segmentos acionáveis como:\n","\n","| Segmento | Pontuação RFM (Exemplo) | Perfil do Cliente | Ação Estratégica Sugerida |\n","| :--- | :--- | :--- | :--- |\n","| **Campeões** | 444 | Nossos melhores clientes: compram com frequência, recentemente e gastam muito. | Recompensar com ofertas exclusivas, não apenas descontos. |\n","| **Clientes Leais** | X4X (ex: 343, 442) | Compram com frequência, mas talvez não gastem tanto ou não compraram tão recentemente. | Reativar com bônus ou frete grátis para manter a lealdade. |\n","| **Em Risco** | 1XX (ex: 144, 134) | Eram bons clientes (alta F e M), mas não compram há muito tempo (baixa Recência). | **Alvo Perfeito para o cupom!** Precisam de um incentivo forte para retornarem. |\n","| **Hibernando** | 111, 212, etc. | Clientes com baixa frequência, baixo gasto e que não compram há muito tempo. | Campanha de reativação com desconto agressivo ou comunicação de baixo custo. |"],"metadata":{"id":"uxOZiBJvnoFx"}},{"cell_type":"markdown","source":["O load para camada silver do df de orders ocorreu antes do que o df de consumers. Apesar de termos tratado isso no load de orders, podemos novamente ter casos de orders apontando para consumers que nao existem no df de consumers tratado."],"metadata":{"id":"5CiRX1v4E9_F"}},{"cell_type":"code","source":["print(\"\\n--- Bloco de Verificação: customer_id em dfp_orders existe em dfp_consumers ---\")\n","\n","# Coleta todos os customer_id únicos do DataFrame dfp_orders.\n","# Utiliza `collect()` para trazer os resultados para o driver,\n","# o que é adequado se a lista de IDs não for excessivamente grande.\n","# Caso contrário, seria melhor fazer a verificação no Spark.\n","order_customer_ids = set([row.customer_id for row in dfp_orders.select(\"customer_id\").distinct().collect()])\n","\n","# Coleta todos os customer_id únicos do DataFrame dfp_consumers.\n","consumer_ids = set([row.customer_id for row in dfp_consumers.select(\"customer_id\").distinct().collect()])\n","\n","# Encontra os customer_id que estão em dfp_orders mas não em dfp_consumers.\n","missing_customer_ids = order_customer_ids - consumer_ids\n","\n","# Verifica se a lista de IDs faltantes está vazia.\n","if not missing_customer_ids:\n","    print(\"✅ Todos os customer_id referenciados em dfp_orders existem em dfp_consumers.\")\n","else:\n","    print(f\"❌ Encontrados {len(missing_customer_ids)} customer_id em dfp_orders que NÃO existem em dfp_consumers.\")\n","    # Opcional: Imprimir alguns dos IDs faltantes para inspeção\n","    # print(\"Exemplos de customer_id faltantes:\", list(missing_customer_ids)[:10])\n","\n","print(\"--- Verificação concluída. ---\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sf3Dg1P_rk98","executionInfo":{"status":"ok","timestamp":1750433648091,"user_tz":180,"elapsed":83976,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"a4e3c7c9-1407-4372-e9a7-63f976f560e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Bloco de Verificação: customer_id em dfp_orders existe em dfp_consumers ---\n","❌ Encontrados 1597 customer_id em dfp_orders que NÃO existem em dfp_consumers.\n","--- Verificação concluída. ---\n"]}]},{"cell_type":"code","source":["print(\"\\n--- Filtrando dfp_orders para manter apenas customer_id válidos ---\")\n","\n","# Seleciona apenas os customer_id válidos da base de consumidores\n","valid_customers_df = dfp_consumers.select(\"customer_id\").distinct()\n","\n","# Realiza um inner join entre dfp_orders_cleaned e os customer_id válidos\n","dfp_orders_filtered = dfp_orders.join(\n","    valid_customers_df,\n","    on=\"customer_id\",\n","    how=\"inner\"\n",")\n","\n","# Verificação de quantos registros foram removidos\n","original_count = dfp_orders.count()\n","filtered_count = dfp_orders_filtered.count()\n","removed_count = original_count - filtered_count\n","\n","print(f\"Filtragem concluída. {removed_count} registros com customer_id inválidos foram removidos.\")\n","print(f\"Total original: {original_count}\")\n","print(f\"Total após filtragem: {filtered_count}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hqPmoRkmFPY4","executionInfo":{"status":"ok","timestamp":1750531614610,"user_tz":180,"elapsed":42464,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"f7b22f2d-ae00-47a1-f787-2f75963980ad"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Filtrando dfp_orders para manter apenas customer_id válidos ---\n","Filtragem concluída. 5936 registros com customer_id inválidos foram removidos.\n","Total original: 2426590\n","Total após filtragem: 2420654\n"]}]},{"cell_type":"markdown","source":["Df bases\n"],"metadata":{"id":"RfGmzWEuf9tc"}},{"cell_type":"code","source":["# --- Bloco 1: Criação do DataFrame de Análise Base (dfp_analysis) ---\n","print(\"--- Bloco 1: Criando o DataFrame de Análise Base (do Desafio 1) ---\")\n","\n","# 1a. Contar pedidos por cliente\n","dfp_customer_orders = dfp_orders_filtered.groupBy(\"customer_id\").agg(\n","    count(\"order_id\").alias(\"order_count\")\n",")\n","\n","# 1b. Juntar com a base do teste A/B\n","dfp_analysis = dfp_ab_test.join(\n","    dfp_customer_orders,\n","    on=\"customer_id\",\n","    how=\"inner\" # Usamos 'inner' pois já validamos que todos no teste têm pedidos\n",")\n","\n","# 1c. Criar a flag de retenção (nosso KPI primário)\n","dfp_analysis = dfp_analysis.withColumn(\n","    \"is_retained\",\n","    when(col(\"order_count\") >= 2, True).otherwise(False)\n",")\n","print(\"DataFrame 'dfp_analysis' criado com sucesso.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5IrDvPdQf_5G","executionInfo":{"status":"ok","timestamp":1750531615030,"user_tz":180,"elapsed":423,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"1f2a7f22-c2c1-43bf-8670-6cdb95826aef"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Bloco 1: Criando o DataFrame de Análise Base (do Desafio 1) ---\n","DataFrame 'dfp_analysis' criado com sucesso.\n"]}]},{"cell_type":"code","source":["# --- Passo 0: Importações Necessárias ---\n","from pyspark.sql.functions import col, count, when, dayofweek, sum, lit, max, datediff\n","# Classes de Machine Learning para preparação dos dados e o teste estatístico\n","from pyspark.ml.feature import StringIndexer, VectorAssembler\n","from pyspark.ml.stat import ChiSquareTest\n","import math\n","import pandas as pd"],"metadata":{"id":"AM-5FJi7mRxk","executionInfo":{"status":"ok","timestamp":1750531615045,"user_tz":180,"elapsed":2,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["Na mesma linha do que foi feito na limpeza para carregar a camada silver, os dados orfaos serao removidos"],"metadata":{"id":"1pjfLOT7FQY3"}},{"cell_type":"code","source":["# --- Bloco 2: Criação da Segmentação por Padrão de Consumo ---\n","print(\"\\n--- Bloco 2: Criando a Segmentação por Padrão de Consumo ---\")\n","\n","# 2a. Taguear Pedidos por Dia da Semana\n","dfp_orders_tagged = dfp_orders_filtered.withColumn(\n","    \"day_of_week\", dayofweek(col(\"order_created_at\"))\n",").withColumn(\n","    \"is_weekend\",\n","    when(col(\"day_of_week\").isin([1, 6, 7]), 1).otherwise(0) # 1=Dom, 6=Sex, 7=Sáb\n",")\n","\n","# 2b. Calcular a Proporção de Pedidos por Cliente\n","dfp_customer_pattern = dfp_orders_tagged.groupBy(\"customer_id\").agg(\n","    count(\"*\").alias(\"total_orders\"),\n","    sum(\"is_weekend\").alias(\"weekend_orders\")\n",")\n","dfp_customer_pattern = dfp_customer_pattern.withColumn(\n","    \"weekend_proportion\",\n","    col(\"weekend_orders\") / col(\"total_orders\")\n",")\n","\n","# 2c. Atribuir os Segmentos Finais com base na proporção\n","WEEKEND_THRESHOLD = 0.69\n","WEEKDAY_THRESHOLD = 0.31\n","dfp_final_segmentation_pattern = dfp_customer_pattern.withColumn(\n","    \"consumption_segment\",\n","    when(col(\"weekend_proportion\") > WEEKEND_THRESHOLD, lit(\"1\"))\n","    .when(col(\"weekend_proportion\") < WEEKDAY_THRESHOLD, lit(\"2\"))\n","    .otherwise(lit(\"3\"))\n",").select(\"customer_id\", \"consumption_segment\")\n","print(\"DataFrame 'dfp_final_segmentation_pattern' criado com sucesso.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GXsmXgHEoHbx","executionInfo":{"status":"ok","timestamp":1750531615613,"user_tz":180,"elapsed":566,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"ad073b2a-cb9e-4ef5-f3e6-6ed52692f0d3"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Bloco 2: Criando a Segmentação por Padrão de Consumo ---\n","DataFrame 'dfp_final_segmentation_pattern' criado com sucesso.\n"]}]},{"cell_type":"code","source":["# --- Bloco 3: Criação do DataFrame Master para Análise de Consumo ---\n","print(\"\\n--- Bloco 3: Unindo os dados para criar o dfp_master_consumption ---\")\n","dfp_master_consumption = dfp_analysis.join(\n","    dfp_final_segmentation_pattern, \"customer_id\", \"inner\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b7TcRcTQgaPZ","executionInfo":{"status":"ok","timestamp":1750531615736,"user_tz":180,"elapsed":125,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"e1bdd5d0-bd04-4357-b203-608e7916cfb0"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Bloco 3: Unindo os dados para criar o dfp_master_consumption ---\n"]}]},{"cell_type":"code","source":["# --- Passo 4: Verificar a Distribuição dos Segmentos ---\n","# É uma boa prática contar quantos clientes caíram em cada segmento.\n","print(\"\\n--- Passo 4: Distribuição de Clientes por Segmento de Consumo ---\")\n","dfp_final_segmentation_pattern.groupBy(\"consumption_segment\").count().show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dsHSb7KtolAa","executionInfo":{"status":"ok","timestamp":1750531641588,"user_tz":180,"elapsed":25836,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"36ca3c59-1fd4-42e2-c549-daf27279b34c"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Passo 4: Distribuição de Clientes por Segmento de Consumo ---\n","+-------------------+------+\n","|consumption_segment| count|\n","+-------------------+------+\n","|                  3|235943|\n","|                  1|212873|\n","|                  2|355743|\n","+-------------------+------+\n","\n"]}]},{"cell_type":"code","source":["dfp_final_segmentation_pattern.show(5,truncate=False)\n","\n","dfp_final_segmentation_pattern.select(\"customer_id\").distinct().count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jhTEKPm0q279","executionInfo":{"status":"ok","timestamp":1750435584778,"user_tz":180,"elapsed":30619,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"b1989da4-824e-4edf-8349-c7fa126ae23f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------------------------------------------------------------+-------------------+\n","|customer_id                                                     |consumption_segment|\n","+----------------------------------------------------------------+-------------------+\n","|00006f567cb362ba98b0a23d9f9f73122e9ad98c9edb45bf2d5512068c2d1bf3|2                  |\n","|0000bb10fb47a1d6b2d73754ef383950ef536c77d07212e431c3ff77f68834c9|1                  |\n","|0000c21984ae00cefb5d4931bfa49483dde546413c9b40c4228220f27d7ecdf2|3                  |\n","|0001226e5175177581c0e520732ba58a61dfa96d57be0caef75f5bc948887a8c|1                  |\n","|0001274ea3bc24cee339c5bfe5c579c9175cb9a3a5050334d16ab68dcc5784aa|1                  |\n","+----------------------------------------------------------------+-------------------+\n","only showing top 5 rows\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["804559"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["dfp_master_consumption.show(5,truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ou8LF0llhz0l","executionInfo":{"status":"ok","timestamp":1750435619504,"user_tz":180,"elapsed":34729,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"0187c876-9672-45d6-9824-e66d012c7283"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------------------------------------------------------------+---------+-----------------------+--------------------+-----------+-----------+-------------------+\n","|customer_id                                                     |is_target|created_at             |customer_phone_state|order_count|is_retained|consumption_segment|\n","+----------------------------------------------------------------+---------+-----------------------+--------------------+-----------+-----------+-------------------+\n","|00006f567cb362ba98b0a23d9f9f73122e9ad98c9edb45bf2d5512068c2d1bf3|target   |2018-04-06 03:20:52.886|NaN                 |5          |true       |2                  |\n","|0000c21984ae00cefb5d4931bfa49483dde546413c9b40c4228220f27d7ecdf2|control  |2018-01-07 14:36:55.865|NaN                 |23         |true       |3                  |\n","|0001226e5175177581c0e520732ba58a61dfa96d57be0caef75f5bc948887a8c|target   |2018-03-31 23:13:40.808|RJ                  |2          |true       |1                  |\n","|00021cd56b6d6c980c3b48ca0fdf8c53cf3fea776d74ab401d02b84a5bc1bbad|target   |2018-01-04 15:16:01.061|PE                  |3          |true       |2                  |\n","|00021f6dc15d10418b7765cdcdb18c5535c753e53b42f1278b762b3b94c982b5|target   |2018-01-30 21:16:00.322|RS                  |2          |true       |2                  |\n","+----------------------------------------------------------------+---------+-----------------------+--------------------+-----------+-----------+-------------------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","source":["dfp_master_consumption.groupBy(\"is_target\",\"consumption_segment\",\"is_retained\").count().orderBy(\"is_target\", \"consumption_segment\", \"is_retained\").show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N40k4J-KptoB","executionInfo":{"status":"ok","timestamp":1750437308174,"user_tz":180,"elapsed":27201,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"8acf0682-fe69-4d51-f1ab-4f13533e0d7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+-------------------+-----------+------+\n","|is_target|consumption_segment|is_retained| count|\n","+---------+-------------------+-----------+------+\n","|  control|                  1|      false| 78911|\n","|  control|                  1|       true| 22081|\n","|  control|                  2|      false|109493|\n","|  control|                  2|       true| 54670|\n","|  control|                  3|       true| 94544|\n","|   target|                  1|      false| 78729|\n","|   target|                  1|       true| 33152|\n","|   target|                  2|      false|109867|\n","|   target|                  2|       true| 81713|\n","|   target|                  3|       true|141399|\n","+---------+-------------------+-----------+------+\n","\n"]}]},{"cell_type":"markdown","source":["## Conclusão da Segmentação por Padrão de Consumo\n","\n","A execução do código de segmentação por padrão de consumo foi bem-sucedida e nos forneceu a primeira camada de inteligência sobre a nossa base de usuários. A distribuição dos clientes nos três segmentos criados foi a seguinte:\n","\n","* **Usuários de Dias de Semana:** ~355k clientes\n","* **Usuários Híbridos/Intensivos:** ~235k clientes\n","* **Usuários de Fim de Semana:** ~212k clientes\n","\n","### Principais Insights da Análise\n","\n","Esta divisão inicial já revela insights de negócio valiosos:\n","\n","1.  **A Força do Hábito da Semana:** O maior segmento, de forma clara, é o de \"Usuários de Dias de Semana\". Isso sugere que, para uma grande parte da base, o iFood desempenha um papel fundamental como uma **solução de conveniência para a rotina**, como almoços durante o trabalho ou jantares práticos, e não apenas como um serviço para ocasiões de lazer.\n","\n","2.  **O Valor do Usuário Intensivo:** O segundo maior grupo é o de \"Usuários Híbridos/Intensivos\". Este é um segmento extremamente importante, pois representa os clientes mais engajados, que integraram o iFood em todos os momentos de sua semana, indicando alta lealdade.\n","\n","3.  **Desmistificando o \"Foco no Fim de Semana\":** Embora ainda seja um grupo grande, os \"Usuários de Fim de Semana\" representam o menor dos três segmentos. Isso desafia a possível percepção de que o iFood é uma plataforma predominantemente de lazer e indica a força do seu caso de uso prático no dia a dia.\n","\n","> Essa clara distinção nos perfis de consumo é fundamental. Ela nos permitirá, na próxima etapa, analisar como a campanha de cupons impactou cada um desses grupos de forma diferente, abrindo caminho para estratégias de marketing muito mais personalizadas e eficientes."],"metadata":{"id":"Qghfxef5pU6p"}},{"cell_type":"code","source":["# Mapeamento dos códigos para nomes legíveis\n","map_pattern = {'1': 'Usuário de Fim de Semana', '2': 'Usuário de Dias de Semana', '3': 'Usuário Híbrido/Intensivo'}\n","\n","# Coletamos os códigos dos segmentos para iterar\n","segments_to_analyze = [row['consumption_segment'] for row in dfp_master_consumption.select(\"consumption_segment\").distinct().collect()]\n","\n","# Loop de Análise\n","for segment_code in segments_to_analyze:\n","    segment_name = map_pattern.get(segment_code, \"Desconhecido\")\n","\n","    print(\"\\n\" + \"=\"*50)\n","    print(f\"### Iniciando Análise para o Segmento: {segment_name} ###\")\n","    print(\"=\"*50)\n","\n","    # Filtra o DataFrame para o segmento atual\n","    df_segment = dfp_master_consumption.filter(col(\"consumption_segment\") == segment_code)\n","\n","    # Prepara os dados para o ChiSquareTest do PySpark\n","    # 1. Converte a flag de retenção para inteiro (0 ou 1)\n","    df_segment = df_segment.withColumn(\"is_retained_int\", col(\"is_retained\").cast(\"integer\"))\n","\n","    # 2. Converte a coluna de grupo 'is_target' para um índice numérico 'label'\n","    label_indexer = StringIndexer(inputCol=\"is_target\", outputCol=\"label\")\n","    df_segment = label_indexer.fit(df_segment).transform(df_segment)\n","\n","    # 3. Monta a coluna de feature em um formato de vetor\n","    feature_assembler = VectorAssembler(inputCols=[\"is_retained_int\"], outputCol=\"features\")\n","    dfp_prepared = feature_assembler.transform(df_segment)\n","\n","    # --- Executando o Teste Qui-Quadrado ---\n","    # Usamos um try-except para o caso de um segmento não ter dados suficientes\n","    try:\n","        print(f\"\\n--- Teste Qui-Quadrado para o segmento '{segment_name}' ---\")\n","        chi_square_result = ChiSquareTest.test(dfp_prepared, \"features\", \"label\").first()\n","\n","        # Extraindo os resultados\n","        p_value = chi_square_result.pValues[0]\n","        chi2_statistic = chi_square_result.statistics[0]\n","\n","        print(f\"P-Valor do teste: {p_value:.8f}\")\n","        print(f\"Estatística Qui-Quadrado: {chi2_statistic:.4f}\")\n","\n","        # Veredito estatístico\n","        alpha = 0.05\n","        if p_value < alpha:\n","            print(\"Veredito: A diferença é ESTATISTICAMENTE SIGNIFICATIVA (Rejeitamos H0).\")\n","        else:\n","            print(\"Veredito: A diferença NÃO é estatisticamente significativa (Não podemos rejeitar H0).\")\n","\n","    except Exception as e:\n","        print(f\"AVISO: O teste não pôde ser executado para o segmento '{segment_name}'. Causa: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MJ7w0GkzGNuP","executionInfo":{"status":"ok","timestamp":1750531888990,"user_tz":180,"elapsed":247344,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"aef8592e-0775-414e-d638-b30010313dad"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==================================================\n","### Iniciando Análise para o Segmento: Usuário Híbrido/Intensivo ###\n","==================================================\n","\n","--- Teste Qui-Quadrado para o segmento 'Usuário Híbrido/Intensivo' ---\n","P-Valor do teste: 1.00000000\n","Estatística Qui-Quadrado: 0.0000\n","Veredito: A diferença NÃO é estatisticamente significativa (Não podemos rejeitar H0).\n","\n","==================================================\n","### Iniciando Análise para o Segmento: Usuário de Fim de Semana ###\n","==================================================\n","\n","--- Teste Qui-Quadrado para o segmento 'Usuário de Fim de Semana' ---\n","P-Valor do teste: 0.00000000\n","Estatística Qui-Quadrado: 1666.6616\n","Veredito: A diferença é ESTATISTICAMENTE SIGNIFICATIVA (Rejeitamos H0).\n","\n","==================================================\n","### Iniciando Análise para o Segmento: Usuário de Dias de Semana ###\n","==================================================\n","\n","--- Teste Qui-Quadrado para o segmento 'Usuário de Dias de Semana' ---\n","P-Valor do teste: 0.00000000\n","Estatística Qui-Quadrado: 3269.3172\n","Veredito: A diferença é ESTATISTICAMENTE SIGNIFICATIVA (Rejeitamos H0).\n"]}]},{"cell_type":"markdown","source":["## Análise dos Resultados Estatísticos por Padrão de Consumo\n","\n","A análise estatística do teste A/B para cada segmento de consumo revela um insight fundamental: **o impacto da campanha de cupons varia drasticamente dependendo do perfil de uso do cliente.** Os resultados mostram que a campanha foi eficaz para alguns grupos, mas completamente inócua para outros.\n","\n","### Usuários de Fim de Semana e de Dias de Semana: Impacto Significativo\n","\n","Para estes dois segmentos, a análise estatística confirma que a campanha funcionou.\n","\n","* **Resultado:** Tanto para \"Usuários de Fim de Semana\" quanto para \"Usuários de Dias de Semana\", o p-valor foi `0.0000`, o que nos leva a **rejeitar a Hipótese Nula** com o mais alto grau de confiança.\n","* **Interpretação:** Isso significa que, para ambos os perfis, o cupom foi um **incentivo eficaz para aumentar a retenção**. Os clientes que concentram seus pedidos ou na rotina da semana ou nos momentos de lazer do fim de semana foram positivamente influenciados pelo desconto.\n","* **Observação Adicional:** É notável que a estatística Qui-Quadrado para o grupo de \"Dias de Semana\" (`3269`) é quase o dobro da do grupo de \"Fim de Semana\" (`1656.0`). Isso sugere que o efeito da campanha pode ter sido ainda mais forte e pronunciado nos usuários com perfil de consumo de rotina.\n","\n","### Usuários Híbridos/Intensivos: Ausência de Impacto\n","\n","O resultado para este segmento é o mais revelador da análise.\n","\n","* **Resultado:** O p-valor de `1.0000` e a Estatística Qui-Quadrado de `0.0000` indicam que não houve **absolutamente nenhuma diferença** no comportamento de retenção entre os grupos controle e teste dentro deste segmento.\n","* **Interpretação:** O cupom foi **completamente ineficaz** para alterar o comportamento deste grupo. A explicação mais provável é que esses clientes, por já serem altamente engajados e utilizarem o iFood em todos os contextos (dias de semana e fins de semana), já possuem um hábito de consumo consolidado. Sua decisão de compra provavelmente é movida por necessidade e conveniência, e não por um incentivo pontual como um cupom.\n","\n","### Implicações Estratégicas Iniciais\n","\n","> A conclusão desta análise estatística é clara: o cupom não é uma ferramenta universal, mas sim uma alavanca de **mudança de comportamento**. Ele é poderoso para influenciar clientes com padrões definidos, mas ineficaz para aqueles que já atingiram um alto patamar de engajamento. Este é um insight crucial para a otimização de custos: oferecer este tipo de cupom para o segmento \"Híbrido/Intensivo\" com o objetivo de aumentar a retenção é um investimento com retorno nulo."],"metadata":{"id":"T2xbHpK698Ha"}},{"cell_type":"code","source":["# --- Bloco 4: Análise de Impacto de Negócio por Segmento de Consumo ---\n","\n","print(\"\\n\\n\" + \"=\"*80)\n","print(\"### ETAPA 2: ANÁLISE DE NEGÓCIO E IMPACTO POR PADRÃO DE CONSUMO ###\")\n","print(\"=\"*80)\n","\n","# Mapeamento dos códigos para nomes legíveis\n","map_pattern = {'1': 'Usuário de Fim de Semana', '2': 'Usuário de Dias de Semana', '3': 'Usuário Híbrido/Intensivo'}\n","\n","# Coletamos os códigos dos segmentos para iterar\n","segments_to_analyze = [row['consumption_segment'] for row in dfp_master_consumption.select(\"consumption_segment\").distinct().collect()]\n","\n","# Lista para armazenar o resumo final\n","final_business_results = []\n","\n","# Loop de Análise de Negócio\n","for segment_code in sorted(segments_to_analyze):\n","    segment_name = map_pattern.get(segment_code, \"Desconhecido\")\n","\n","    print(\"\\n\" + \"-\"*60)\n","    print(f\"### Calculando Métricas de Negócio para o Segmento: {segment_name} ###\")\n","    print(\"-\"*60)\n","\n","    # Filtra o DataFrame para o segmento atual\n","    df_segment = dfp_master_consumption.filter(col(\"consumption_segment\") == segment_code)\n","\n","    # Agregação para obter os números da tabela de contingência\n","    summary_df = df_segment.groupBy(\"is_target\").agg(\n","        count(\"*\").alias(\"total_users\"),\n","        count(when(col(\"is_retained\") == True, True)).alias(\"retained_users\")\n","    ).collect()\n","\n","    # Verifica se temos ambos os grupos para comparar\n","    control_rows = [row for row in summary_df if row['is_target'] == 'control']\n","    target_rows = [row for row in summary_df if row['is_target'] == 'target']\n","\n","    if not control_rows or not target_rows:\n","        print(f\"AVISO: Dados insuficientes para o segmento '{segment_name}'. Análise pulada.\")\n","        continue\n","\n","    control_row = control_rows[0]\n","    target_row = target_rows[0]\n","\n","    control_total = control_row['total_users']\n","    control_retained = control_row['retained_users']\n","    target_total = target_row['total_users']\n","    target_retained = target_row['retained_users']\n","\n","    # Calcula as taxas de retenção\n","    retention_rate_control = control_retained / control_total if control_total > 0 else 0\n","    retention_rate_target = target_retained / target_total if target_total > 0 else 0\n","\n","    print(f\"Total de Usuários (Controle): {control_total}\")\n","    print(f\"Usuários Retidos (Controle): {control_retained}\")\n","    print(f\"Taxa de Retenção (Controle): {retention_rate_control:.2%}\")\n","    print(\"-\" * 30)\n","    print(f\"Total de Usuários (Teste): {target_total}\")\n","    print(f\"Usuários Retidos (Teste): {target_retained}\")\n","    print(f\"Taxa de Retenção (Teste): {retention_rate_target:.2%}\")\n","\n","    # Calcula o Lift\n","    lift = ((retention_rate_target - retention_rate_control) / retention_rate_control) * 100 if retention_rate_control > 0 else 0\n","    print(f\"\\nLift Percentual (Aumento): {lift:.2f}%\")\n","\n","    # Calcula o Intervalo de Confiança para a diferença das proporções\n","    diff = retention_rate_target - retention_rate_control\n","    if target_total > 0 and control_total > 0:\n","        # Evita erro de divisão por zero no cálculo do erro padrão\n","        std_dev1_sq = retention_rate_target * (1 - retention_rate_target) / target_total\n","        std_dev2_sq = retention_rate_control * (1 - retention_rate_control) / control_total\n","        std_err_diff = math.sqrt(std_dev1_sq + std_dev2_sq)\n","        conf_interval_diff = (diff - 1.96 * std_err_diff, diff + 1.96 * std_err_diff)\n","    else:\n","        conf_interval_diff = (0, 0)\n","\n","    print(f\"Diferença Absoluta: {diff:.2%}\")\n","    print(f\"Intervalo de Confiança 95% para a Diferença: [{conf_interval_diff[0]:.2%}, {conf_interval_diff[1]:.2%}]\")\n","\n","    # Armazena o resultado para a tabela final\n","    final_business_results.append({\n","        'Segmento': segment_name,\n","        'Retenção (Controle)': retention_rate_control,\n","        'Retenção (Teste)': retention_rate_target,\n","        'Lift (%)': lift\n","    })\n","\n","# --- Bloco 5: Apresentação da Tabela Resumo ---\n","print(\"\\n\\n\" + \"=\"*80)\n","print(\"### TABELA RESUMO: MÉTRICAS DE NEGÓCIO POR PADRÃO DE CONSUMO ###\")\n","print(\"=\"*80)\n","\n","df_final_biz = pd.DataFrame(final_business_results)\n","\n","# Formatando as colunas\n","df_final_biz['Retenção (Controle)'] = df_final_biz['Retenção (Controle)'].apply(lambda x: f'{x:.2%}')\n","df_final_biz['Retenção (Teste)'] = df_final_biz['Retenção (Teste)'].apply(lambda x: f'{x:.2%}')\n","df_final_biz['Lift (%)'] = df_final_biz['Lift (%)'].apply(lambda x: f'{x:.2f}%')\n","\n","# Ordenando por Lift para destacar os segmentos com maior impacto\n","print(df_final_biz.sort_values(by='Lift (%)', key=lambda x: x.str.replace('%', '').astype(float), ascending=False).to_string(index=False))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MmCMZCvLcfnN","executionInfo":{"status":"ok","timestamp":1750532002559,"user_tz":180,"elapsed":113570,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"1f65592d-126f-43e7-b388-db97cdd34bd0"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","================================================================================\n","### ETAPA 2: ANÁLISE DE NEGÓCIO E IMPACTO POR PADRÃO DE CONSUMO ###\n","================================================================================\n","\n","------------------------------------------------------------\n","### Calculando Métricas de Negócio para o Segmento: Usuário de Fim de Semana ###\n","------------------------------------------------------------\n","Total de Usuários (Controle): 100992\n","Usuários Retidos (Controle): 22081\n","Taxa de Retenção (Controle): 21.86%\n","------------------------------\n","Total de Usuários (Teste): 111881\n","Usuários Retidos (Teste): 33152\n","Taxa de Retenção (Teste): 29.63%\n","\n","Lift Percentual (Aumento): 35.53%\n","Diferença Absoluta: 7.77%\n","Intervalo de Confiança 95% para a Diferença: [7.40%, 8.14%]\n","\n","------------------------------------------------------------\n","### Calculando Métricas de Negócio para o Segmento: Usuário de Dias de Semana ###\n","------------------------------------------------------------\n","Total de Usuários (Controle): 164163\n","Usuários Retidos (Controle): 54670\n","Taxa de Retenção (Controle): 33.30%\n","------------------------------\n","Total de Usuários (Teste): 191580\n","Usuários Retidos (Teste): 81713\n","Taxa de Retenção (Teste): 42.65%\n","\n","Lift Percentual (Aumento): 28.08%\n","Diferença Absoluta: 9.35%\n","Intervalo de Confiança 95% para a Diferença: [9.03%, 9.67%]\n","\n","------------------------------------------------------------\n","### Calculando Métricas de Negócio para o Segmento: Usuário Híbrido/Intensivo ###\n","------------------------------------------------------------\n","Total de Usuários (Controle): 94544\n","Usuários Retidos (Controle): 94544\n","Taxa de Retenção (Controle): 100.00%\n","------------------------------\n","Total de Usuários (Teste): 141399\n","Usuários Retidos (Teste): 141399\n","Taxa de Retenção (Teste): 100.00%\n","\n","Lift Percentual (Aumento): 0.00%\n","Diferença Absoluta: 0.00%\n","Intervalo de Confiança 95% para a Diferença: [0.00%, 0.00%]\n","\n","\n","================================================================================\n","### TABELA RESUMO: MÉTRICAS DE NEGÓCIO POR PADRÃO DE CONSUMO ###\n","================================================================================\n","                 Segmento Retenção (Controle) Retenção (Teste) Lift (%)\n"," Usuário de Fim de Semana              21.86%           29.63%   35.53%\n","Usuário de Dias de Semana              33.30%           42.65%   28.08%\n","Usuário Híbrido/Intensivo             100.00%          100.00%    0.00%\n"]}]},{"cell_type":"markdown","source":["### Análise das Métricas de Negócio e Conclusão: Impacto por Padrão de Consumo\n","\n","Com a verificação estatística conduzida para cada grupo segmentado por padrão de consumo, esta etapa quantifica o impacto **granular da campanha** nos diferentes perfis de usuários, oferecendo uma leitura mais direcionada e prática dos resultados.\n","\n","#### Usuários de Fim de Semana\n","\n","- **Taxa de Retenção (Controle):** 21.86%\n","- **Taxa de Retenção (Teste):** 29.63%\n","- **Lift Percentual:** +35.53%\n","- **Diferença Absoluta:** +7.77 pontos percentuais\n","- **Intervalo de Confiança 95%:** [7.40%, 8.14%]\n","\n","Este é um resultado altamente expressivo. O grupo de usuários mais ativos nos fins de semana respondeu muito bem à campanha, com **ganhos consistentes e estatisticamente significativos**.\n","\n","#### Usuários de Dias de Semana\n","\n","- **Taxa de Retenção (Controle):** 33.30%\n","- **Taxa de Retenção (Teste):** 42.65%\n","- **Lift Percentual:** +28.08%\n","- **Diferença Absoluta:** +9.35 pontos percentuais\n","- **Intervalo de Confiança 95%:** [9.03%, 9.67%]\n","\n","Os resultados para esse grupo também confirmam um **impacto positivo e robusto** da campanha, com ganhos sólidos tanto em termos absolutos quanto relativos, novamente com um intervalo de confiança inteiramente positivo.\n","\n","#### Usuários Híbrido/Intensivo\n","\n","- **Taxa de Retenção (Controle):** 100.00%\n","- **Taxa de Retenção (Teste):** 100.00%\n","- **Lift Percentual:** 0.00%\n","- **Diferença Absoluta:** 0.00 pontos percentuais\n","- **Intervalo de Confiança 95%:** [0.00%, 0.00%]\n","\n","Este resultado, à primeira vista, sugere uma retenção perfeita para ambos os grupos — algo que, no contexto real, **não é plausível**. Essa anomalia pode ter sido causada por:\n","\n","- Uma **distorção nos dados de origem** (ex: erro de processamento, sampleagem enviesada)\n","- Um problema nas **premissas de segregação dos grupos**\n","- Ou ainda uma definição equivocada de retenção para esse perfil específico\n","\n","Em um ambiente de produção, esse segmento exigiria **investigação aprofundada** e revisão dos critérios de segmentação e cálculo. No contexto atual, optamos por **não considerar esse segmento** na conclusão final.\n","\n","### Conclusão Geral por Segmento\n","\n","> Com base nos dois primeiros segmentos — **Usuários de Fim de Semana** e **Usuários de Dias de Semana** — podemos afirmar com alta confiança que a campanha de cupons gerou **efeitos positivos, significativos e relevantes do ponto de vista de negócio**. O Lift Percentual em ambos os casos foi superior a 28%, com diferenças absolutas claras e estatisticamente sustentadas.\n",">\n","> Apesar da anomalia no grupo Híbrido/Intensivo, os demais resultados reforçam a conclusão de que a campanha **funciona de forma especialmente eficaz** para perfis de uso bem definidos, validando sua eficácia como alavanca de retenção segmentada.\n"],"metadata":{"id":"1yJfhaA5Nwca"}},{"cell_type":"markdown","source":["Segmentação por Valor e Engajamento"],"metadata":{"id":"d1pQzzrxpSJb"}},{"cell_type":"code","source":["# --- Passo 0: Importações Necessárias ---\n","from pyspark.sql.functions import col, count, sum, max, datediff, lit, concat, ntile, date_add\n","from pyspark.sql.window import Window\n"],"metadata":{"id":"xIdL8W62pf5e","executionInfo":{"status":"ok","timestamp":1750532002579,"user_tz":180,"elapsed":24,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# --- Passo 1: Calcular R, F e M para cada Cliente ---\n","print(\"--- Passo 1: Calculando Recência, Frequência e Valor Monetário ---\")\n","\n","# A Recência precisa de um \"hoje\".\n","# vamos definir uma data de referência logo após o último pedido nos dados.\n","# Em um cenário real, usaríamos a data atual.\n","# Encontra a data máxima e já adiciona 1 dia, tudo dentro do Spark.\n","# A função date_add funciona com colunas do tipo Date ou Timestamp.\n","data_de_referencia_row = dfp_orders_filtered.agg(\n","    date_add(max(\"order_created_at\"), 1).alias(\"ref_date\")\n",").first()\n","data_de_referencia = data_de_referencia_row['ref_date']\n","\n","print(f\"Data de referência para cálculo da Recência (dinâmica): {data_de_referencia}\")\n","\n","dfp_rfm = dfp_orders_filtered.groupBy(\"customer_id\").agg(\n","    # Recência (R): Diferença em dias da data de referência para o último pedido\n","    datediff(lit(data_de_referencia), max(\"order_created_at\")).alias(\"recency\"),\n","    # Frequência (F): Contagem total de pedidos\n","    count(\"order_id\").alias(\"frequency\"),\n","    # Monetário (M): Soma total do valor dos pedidos\n","    sum(\"order_total_amount\").alias(\"monetary\")\n",")\n","\n","dfp_rfm.show(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DrX6abt4pprM","executionInfo":{"status":"ok","timestamp":1750532028417,"user_tz":180,"elapsed":25836,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"8da902c1-ef0b-4d5e-d984-85bafed11d0e"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Passo 1: Calculando Recência, Frequência e Valor Monetário ---\n","Data de referência para cálculo da Recência (dinâmica): 2019-02-01\n","+--------------------+-------+---------+-----------------+\n","|         customer_id|recency|frequency|         monetary|\n","+--------------------+-------+---------+-----------------+\n","|00006f567cb362ba9...|      8|        5|533.0999999999999|\n","|0000c21984ae00cef...|      1|       23|          1011.36|\n","|0001226e517517758...|     48|        2|             86.5|\n","|00021cd56b6d6c980...|     29|        3|             47.8|\n","|00021f6dc15d10418...|     16|        2|            174.6|\n","+--------------------+-------+---------+-----------------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","source":["# --- Passo 2: Calcular os Scores RFM (1 a 4) ---\n","# Usamos a função ntile() sobre uma Window para criar os 4 quartis (grupos de 25%).\n","\n","print(\"\\n--- Passo 2: Criando os scores de R, F e M ---\")\n","\n","# A Window para Frequência e Monetário ordena do maior para o menor (quanto mais, melhor)\n","window_freq_monetary = Window.orderBy(col(\"value\").desc())\n","# A Window para Recência ordena do menor para o maior (quanto menos dias, melhor)\n","window_recency = Window.orderBy(col(\"value\").asc())\n","\n","# Criando os scores\n","score_r = dfp_rfm.select(\"customer_id\", col(\"recency\").alias(\"value\")) \\\n","    .withColumn(\"r_score\", ntile(4).over(window_recency)) \\\n","    .select(\"customer_id\", \"r_score\")\n","\n","score_f = dfp_rfm.select(\"customer_id\", col(\"frequency\").alias(\"value\")) \\\n","    .withColumn(\"f_score\", ntile(4).over(window_freq_monetary)) \\\n","    .select(\"customer_id\", \"f_score\")\n","\n","score_m = dfp_rfm.select(\"customer_id\", col(\"monetary\").alias(\"value\")) \\\n","    .withColumn(\"m_score\", ntile(4).over(window_freq_monetary)) \\\n","    .select(\"customer_id\", \"m_score\")\n","\n","# Juntando os scores em um único DataFrame\n","dfp_rfm_scores = dfp_rfm.join(score_r, \"customer_id\") \\\n","                        .join(score_f, \"customer_id\") \\\n","                        .join(score_m, \"customer_id\")\n","\n","dfp_rfm_scores.select(\"customer_id\", \"recency\", \"r_score\", \"frequency\", \"f_score\", \"monetary\", \"m_score\").show(10)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z-rA2VJZqO0y","executionInfo":{"status":"ok","timestamp":1750532098381,"user_tz":180,"elapsed":69946,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"8bfca1fd-b97e-4480-aa63-87397e651a4c"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Passo 2: Criando os scores de R, F e M ---\n","+--------------------+-------+-------+---------+-------+--------+-------+\n","|         customer_id|recency|r_score|frequency|f_score|monetary|m_score|\n","+--------------------+-------+-------+---------+-------+--------+-------+\n","|0001226e517517758...|     48|      4|        2|      2|    86.5|      2|\n","|00021cd56b6d6c980...|     29|      3|        3|      1|    47.8|      3|\n","|00021f6dc15d10418...|     16|      2|        2|      2|   174.6|      1|\n","|00022b8c0c7af061f...|     19|      2|        5|      1|    80.9|      2|\n","|00029b26fb2121119...|     10|      2|        1|      3|    54.6|      3|\n","|0002cc7394d677fdf...|     49|      4|        1|      3|    43.9|      3|\n","|0003fc1a7fe21c5f4...|      6|      1|        2|      2|    69.0|      3|\n","|000405bb6de6550fe...|      3|      1|        3|      1|  175.07|      1|\n","|0005bc77d13c12d94...|     18|      2|        5|      1|   315.9|      1|\n","|0006aba7fd94d9de3...|     45|      4|        2|      2|   109.9|      2|\n","+--------------------+-------+-------+---------+-------+--------+-------+\n","only showing top 10 rows\n","\n"]}]},{"cell_type":"code","source":["# --- Passo 3: Criar o Score RFM e os Segmentos Finais ---\n","print(\"\\n--- Passo 3: Criando os segmentos RFM finais ---\")\n","\n","# Concatenamos os scores para criar um score RFM final (ex: \"444\")\n","dfp_final_segmentation_rfm = dfp_rfm_scores.withColumn(\n","    \"rfm_score\",\n","    concat(col(\"r_score\"), col(\"f_score\"), col(\"m_score\"))\n",")\n","\n","# Mapeamos os scores para os segmentos de negócio que definimos\n","dfp_final_segmentation_rfm = dfp_final_segmentation_rfm.withColumn(\n","    \"rfm_segment\",\n","    when(col(\"rfm_score\") == \"444\", lit(\"1\"))\n","    .when((col(\"f_score\") == 4) & (col(\"r_score\") >= 3), lit(\"2\"))\n","    .when(col(\"r_score\") <= 2, lit(\"3\")) # Simplificação de usuários que não compram há tempos\n","    .when((col(\"r_score\") >= 3) & (col(\"f_score\") <= 2), lit(\"4\"))\n","    .otherwise(lit(\"5\"))\n",")\n","\n","#grupo campeao = 1\n","#grupo Clientes LEais = 2\n","#grupo \"Em Risco / Hibernando = 3\n","#grupo Novos / Potenciais = 4\n","#grupo Outros = 5\n","\n","# Vamos selecionar as colunas finais e ver o resultado\n","dfp_final_segmentation_rfm = dfp_final_segmentation_rfm.select(\"customer_id\", \"rfm_segment\", \"rfm_score\")\n","dfp_final_segmentation_rfm.show(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1l7suVhBqfML","executionInfo":{"status":"ok","timestamp":1750532150520,"user_tz":180,"elapsed":52141,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"f2db887a-d308-4549-b65e-d26953af8f1b"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Passo 3: Criando os segmentos RFM finais ---\n","+--------------------+-----------+---------+\n","|         customer_id|rfm_segment|rfm_score|\n","+--------------------+-----------+---------+\n","|00006f567cb362ba9...|          3|      211|\n","|0000c21984ae00cef...|          3|      111|\n","|0001226e517517758...|          4|      422|\n","|00021cd56b6d6c980...|          4|      313|\n","|00021f6dc15d10418...|          3|      221|\n","|0002287b123ac1afc...|          5|      433|\n","|00022b8c0c7af061f...|          3|      212|\n","|00027035d16a4de43...|          5|      332|\n","|00029b26fb2121119...|          3|      233|\n","|0002cc7394d677fdf...|          5|      433|\n","+--------------------+-----------+---------+\n","only showing top 10 rows\n","\n"]}]},{"cell_type":"code","source":["# --- Passo 4: Verificar a Distribuição dos Segmentos ---\n","print(\"\\n--- Passo 4: Distribuição de Clientes por Segmento RFM ---\")\n","dfp_final_segmentation_rfm.groupBy(\"rfm_segment\").count().orderBy(col(\"count\").desc()).show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ffu3mEv5qnBn","executionInfo":{"status":"ok","timestamp":1750438153005,"user_tz":180,"elapsed":59296,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"c09aca88-8521-4fda-ec22-e1c93cc30820"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Passo 4: Distribuição de Clientes por Segmento RFM ---\n","+-----------+------+\n","|rfm_segment| count|\n","+-----------+------+\n","|          3|402280|\n","|          5|137219|\n","|          4|122712|\n","|          2|100628|\n","|          1| 41720|\n","+-----------+------+\n","\n"]}]},{"cell_type":"markdown","source":["## Conclusão da Segmentação por Valor e Engajamento (RFM)\n","\n","A aplicação do modelo RFM (Recência, Frequência, Valor Monetário) nos permitiu classificar a base de clientes em segmentos de alto valor estratégico. Esta é a segunda camada da nossa análise, focada em entender o valor e o nível de engajamento de cada usuário.\n","\n","### Recapitulação da Metodologia e Regras Aplicadas\n","\n","Para construir os segmentos, seguimos um processo de 3 passos:\n","\n","1.  **Cálculo das Métricas Individuais:** Para cada cliente, calculamos:\n","    * **Recência (R):** O número de dias entre o último pedido do cliente e a data de referência final do nosso dataset.\n","    * **Frequência (F):** A contagem total de pedidos realizados pelo cliente no período.\n","    * **Valor Monetário (M):** A soma total (em R$) de todos os pedidos realizados pelo cliente.\n","\n","2.  **Atribuição de Scores:** Cada uma dessas três métricas foi dividida em quartis (4 grupos de 25% do total de usuários). Com base nisso, cada cliente recebeu uma pontuação de 1 a 4 para cada métrica (R, F e M). A pontuação 4 representa o melhor quartil (ex: clientes mais recentes, com maior frequência ou maior gasto).\n","\n","3.  **Mapeamento para Segmentos de Negócio:** Os scores individuais foram combinados para formar um \"RFM Score\" (ex: \"444\") e, em seguida, mapeados para segmentos de negócio acionáveis, conforme a regra que definimos.\n","\n","### Distribuição dos Clientes e Insights Estratégicos\n","\n","A distribuição dos clientes nos segmentos criados foi a seguinte:\n","\n","* **Em Risco / Hibernando:** 402280 clientes (~50.0%)\n","* **Outros:** 137219 clientes (~17.0%)\n","* **Novos / Potenciais:** 122712 clientes (~15.3%)\n","* **Clientes Leais:** 100628 clientes (~12.5%)\n","* **Campeões:** 41720 clientes (~5.2%)\n","\n","#### Análise Crítica dos Resultados\n","\n","Esta distribuição revela um panorama claro da base de clientes do iFood neste período:\n","\n","1.  **A Grande Oportunidade de Reativação:** O insight mais impactante é que o maior segmento, correspondendo a **metade da base de clientes**, é o de usuários \"Em Risco / Hibernando\". Trata-se de um grupo massivo de clientes que já demonstraram valor, mas que não compram há tempos. Eles representam a maior oportunidade de crescimento incremental, pois uma campanha de cupons bem direcionada tem um potencial enorme para \"despertá-los\".\n","\n","2.  **O Valioso Núcleo de \"Campeões\":** Como é típico em muitos negócios, um pequeno grupo de clientes (~5%) representa a elite de usuários. Os \"Campeões\" são o ativo mais valioso. A estratégia para eles não deve ser a de oferecer descontos genéricos (que podem canibalizar uma receita já garantida), mas sim a de criar um relacionamento de exclusividade e recompensa pela lealdade.\n","\n","3.  **A Base de Crescimento:** Os segmentos de \"Clientes Leais\" e \"Novos / Potenciais\" formam a espinha dorsal da empresa. São clientes ativos que constituem o \"celeiro\" de onde surgirão os futuros campeões. As estratégias para eles devem focar em aumentar a frequência e o ticket médio para movê-los para o topo da pirâmide de valor.\n","\n"],"metadata":{"id":"RbBQaqNPrwx9"}},{"cell_type":"code","source":["\n","# --- Bloco 1: Preparação para a Análise Segmentada ---\n","print(\"--- Bloco 1: Criando o DataFrame de Análise por Segmento RFM ---\")\n","dfp_master_rfm = dfp_analysis.join(\n","    dfp_final_segmentation_rfm, \"customer_id\", \"inner\"\n",")\n","\n","print(\"Master DataFrame para análise RFM pronto.\")\n","dfp_master_rfm.show(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nHvqGxV3-vqK","executionInfo":{"status":"ok","timestamp":1750532349772,"user_tz":180,"elapsed":58169,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"b3b86495-0db6-45f5-9fa6-ebe979883ba6"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Bloco 1: Criando o DataFrame de Análise por Segmento RFM ---\n","Master DataFrame para análise RFM pronto.\n","+--------------------+---------+--------------------+--------------------+-----------+-----------+-----------+---------+\n","|         customer_id|is_target|          created_at|customer_phone_state|order_count|is_retained|rfm_segment|rfm_score|\n","+--------------------+---------+--------------------+--------------------+-----------+-----------+-----------+---------+\n","|0001226e517517758...|   target|2018-03-31 23:13:...|                  RJ|          2|       true|          4|      422|\n","|00021cd56b6d6c980...|   target|2018-01-04 15:16:...|                  PE|          3|       true|          4|      313|\n","|00021f6dc15d10418...|   target|2018-01-30 21:16:...|                  RS|          2|       true|          3|      221|\n","|00022b8c0c7af061f...|  control|2018-04-06 02:04:...|                  PB|          5|       true|          3|      212|\n","|00029b26fb2121119...|   target|2018-01-03 23:03:...|                  PR|          1|      false|          3|      233|\n","+--------------------+---------+--------------------+--------------------+-----------+-----------+-----------+---------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","source":["# --- Bloco 2: Análise Estatística para Cada Segmento RFM ---\n","# Este bloco itera sobre cada segmento, executa o teste estatístico,\n","# e salva os resultados para apresentar em uma tabela resumo no final.\n","\n","# Pré-requisitos:\n","# - dfp_master_rfm: DataFrame com os dados de teste e a coluna 'rfm_segment'\n","# - Imports: StringIndexer, VectorAssembler, ChiSquareTest, col\n","\n","print(\"\\n\\n\" + \"=\"*80)\n","print(\"### ETAPA 1: ANÁLISE ESTATÍSTICA POR SEGMENTO RFM ###\")\n","print(\"=\"*80)\n","\n","# Mapeamento dos códigos para nomes legíveis\n","map_rfm = {'1': 'Campeões', '2': 'Clientes Leais', '3': 'Em Risco / Hibernando', '4': 'Novos / Potenciais', '5': 'Outros'}\n","\n","# Coletamos os códigos dos segmentos para iterar\n","segments_to_analyze = [row['rfm_segment'] for row in dfp_master_rfm.select(\"rfm_segment\").distinct().collect()]\n","\n","# Lista para armazenar o resumo final dos resultados estatísticos\n","statistical_results_rfm = []\n","\n","# Loop de Análise\n","for segment_code in sorted(segments_to_analyze):\n","    segment_name = map_rfm.get(segment_code, \"Desconhecido\")\n","\n","    print(f\"Processando o segmento: '{segment_name}'...\")\n","\n","    # Filtra o DataFrame para o segmento atual\n","    df_segment = dfp_master_rfm.filter(col(\"rfm_segment\") == segment_code)\n","\n","    # Prepara os dados para o ChiSquareTest do PySpark\n","    df_segment = df_segment.withColumn(\"is_retained_int\", col(\"is_retained\").cast(\"integer\"))\n","    label_indexer = StringIndexer(inputCol=\"is_target\", outputCol=\"label\")\n","    df_segment = label_indexer.fit(df_segment).transform(df_segment)\n","    feature_assembler = VectorAssembler(inputCols=[\"is_retained_int\"], outputCol=\"features\")\n","    dfp_prepared = feature_assembler.transform(df_segment)\n","\n","    # --- Executando o Teste Qui-Quadrado ---\n","    try:\n","        chi_square_result = ChiSquareTest.test(dfp_prepared, \"features\", \"label\").first()\n","\n","        p_value = chi_square_result.pValues[0]\n","        chi2_statistic = chi_square_result.statistics[0]\n","\n","        alpha = 0.05\n","        verdict = \"SIGNIFICATIVA\" if p_value < alpha else \"NÃO SIGNIFICATIVA\"\n","\n","        # Armazena o resultado em vez de imprimir imediatamente\n","        statistical_results_rfm.append({\n","            'Segmento': segment_name,\n","            'P-Valor': p_value,\n","            'Estatística X²': chi2_statistic,\n","            'Veredito (alpha=0.05)': verdict\n","        })\n","\n","    except Exception as e:\n","        print(f\"AVISO: O teste não pôde ser executado para o segmento '{segment_name}'. Causa: {e}\")\n","        # Armazena um resultado de erro\n","        statistical_results_rfm.append({\n","            'Segmento': segment_name,\n","            'P-Valor': np.nan,\n","            'Estatística X²': np.nan,\n","            'Veredito (alpha=0.05)': 'ERRO NO CÁLCULO'\n","        })\n","\n","# --- Apresentação da Tabela Resumo dos Resultados Estatísticos ---\n","print(\"\\n\" + \"=\"*80)\n","print(\"### TABELA RESUMO: RESULTADOS ESTATÍSTICOS POR SEGMENTO RFM ###\")\n","print(\"=\"*80)\n","\n","df_final_stats = pd.DataFrame(statistical_results_rfm)\n","\n","# Formatando as colunas para melhor leitura\n","df_final_stats['P-Valor'] = df_final_stats['P-Valor'].apply(lambda x: f'{x:.4f}' if pd.notna(x) else 'N/A')\n","df_final_stats['Estatística X²'] = df_final_stats['Estatística X²'].apply(lambda x: f'{x:.2f}' if pd.notna(x) else 'N/A')\n","\n","print(df_final_stats.to_string(index=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gyCiXYu7-5H9","executionInfo":{"status":"ok","timestamp":1750533291840,"user_tz":180,"elapsed":942065,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"c7350404-ba52-472b-fb26-0ddc0bacaff7"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","================================================================================\n","### ETAPA 1: ANÁLISE ESTATÍSTICA POR SEGMENTO RFM ###\n","================================================================================\n","Processando o segmento: 'Campeões'...\n","Processando o segmento: 'Clientes Leais'...\n","Processando o segmento: 'Em Risco / Hibernando'...\n","Processando o segmento: 'Novos / Potenciais'...\n","Processando o segmento: 'Outros'...\n","\n","================================================================================\n","### TABELA RESUMO: RESULTADOS ESTATÍSTICOS POR SEGMENTO RFM ###\n","================================================================================\n","             Segmento P-Valor Estatística X² Veredito (alpha=0.05)\n","             Campeões  1.0000           0.00     NÃO SIGNIFICATIVA\n","       Clientes Leais  1.0000           0.00     NÃO SIGNIFICATIVA\n","Em Risco / Hibernando  0.0000        3202.24         SIGNIFICATIVA\n","   Novos / Potenciais  1.0000           0.00     NÃO SIGNIFICATIVA\n","               Outros  0.0000         431.75         SIGNIFICATIVA\n"]}]},{"cell_type":"markdown","source":["## Análise dos Resultados Estatísticos por Segmento RFM\n","\n","A análise estatística do teste A/B para cada segmento RFM revela uma dicotomia clara e poderosa: a campanha de cupons teve um **impacto massivo em clientes de baixo engajamento** e um **impacto nulo em clientes de alto engajamento**. Este é o insight mais estratégico da nossa análise de segmentação.\n","\n","### Segmentos de Baixo Engajamento (\"Em Risco\" e \"Outros\"): Sucesso Absoluto na Reativação\n","\n","* **Resultado:** Os segmentos **\"Em Risco / Hibernando\"** e **\"Outros\"** apresentaram p-valores de `0.0000`, levando a uma rejeição inequívoca da Hipótese Nula. A diferença no comportamento de retenção para estes grupos foi **altamente significativa**.\n","* **Interpretação:** O cupom funcionou como uma ferramenta de **reativação extremamente eficaz**. Para clientes que não compravam há muito tempo ou que tinham um padrão de compra indefinido, o desconto foi o gatilho necessário para trazê-los de volta à plataforma. A estatística Qui-Quadrado massiva para o grupo \"Em Risco\" (`3202`) indica que o efeito foi mais forte neste grupo.\n","\n","### Segmentos de Alto Engajamento (\"Campeões\" e \"Clientes Leais\"): Efeito Nulo\n","\n","* **Resultado:** Os segmentos **\"Campeões\"** e **\"Clientes Leais\"** apresentaram p-valores de `1.0000` e estatísticas Qui-Quadrado de `0.0000`. O veredito é que a diferença foi **estatisticamente não significativa**.\n","* **Interpretação:** O resultado de \"efeito zero\" é um insight crucial. Ele nos diz que o comportamento de compra desses clientes de alto valor **não é influenciado por este tipo de cupom**. Eles já são leais e sua frequência de compra é ditada por hábito e necessidade, não por um desconto pontual. Oferecer este cupom para eles representa uma **canibalização de receita**, ou seja, um custo desnecessário em uma compra que já iria acontecer.\n","\n","### Segmento \"Novos / Potenciais\": Um Insight sobre Ativação\n","\n","* **Resultado:** Assim como os clientes de alto valor, este segmento também mostrou um **efeito nulo** (p-valor de `1.0000`).\n","* **Interpretação:** Isso sugere que, para um usuário que ainda está nas fases iniciais de sua jornada, um cupom de retenção pode não ser o principal motivador para uma segunda compra imediata. Fatores como a qualidade da primeira experiência (entrega, restaurante, produto) podem ter um peso muito maior na decisão de se tornar um cliente recorrente.\n","\n","### Implicações Estratégicas Imediatas\n","\n","> A segmentação RFM nos forneceu um manual de instruções claro para a estratégia de cuponagem do iFood. A campanha não deve ser uma ação de massa, mas sim uma ferramenta cirúrgica:\n",">\n","> 1.  **Foco Total em Reativação:** Os cupons mais agressivos devem ser direcionados ao segmento **\"Em Risco / Hibernando\"**, onde o ROI é potencialmente maior.\n","> 2.  **Mudar a Estratégia para Clientes Topo de Funil:** Para os **\"Campeões\"** e **\"Clientes Leais\"**, a estratégia deve migrar de \"descontos\" para \"recompensas e exclusividade\" (programas de lealdade, benefícios não-financeiros).\n"],"metadata":{"id":"EcGxl7KL_4fk"}},{"cell_type":"code","source":["# --- Bloco 5: Análise de Impacto de Negócio por Segmento RFM ---\n","\n","print(\"\\n\\n\" + \"=\"*80)\n","print(\"### ETAPA 2: ANÁLISE DE NEGÓCIO E IMPACTO POR SEGMENTO RFM ###\")\n","print(\"=\"*80)\n","\n","# Mapeamento dos códigos para nomes legíveis\n","map_rfm = {'1': 'Campeões', '2': 'Clientes Leais', '3': 'Em Risco / Hibernando', '4': 'Novos / Potenciais', '5': 'Outros'}\n","\n","# Coletamos os códigos dos segmentos para iterar\n","segments_to_analyze = [row['rfm_segment'] for row in dfp_master_rfm.select(\"rfm_segment\").distinct().collect()]\n","\n","# Lista para armazenar o resumo final\n","final_business_results_rfm = []\n","\n","# Loop de Análise de Negócio\n","for segment_code in sorted(segments_to_analyze):\n","    segment_name = map_rfm.get(segment_code, \"Desconhecido\")\n","\n","    print(\"\\n\" + \"-\"*60)\n","    print(f\"### Calculando Métricas de Negócio para o Segmento: {segment_name} ###\")\n","    print(\"-\"*60)\n","\n","    # Filtra o DataFrame para o segmento atual\n","    df_segment = dfp_master_rfm.filter(col(\"rfm_segment\") == segment_code)\n","\n","    # Agregação para obter os números da tabela de contingência\n","    summary_df = df_segment.groupBy(\"is_target\").agg(\n","        count(\"*\").alias(\"total_users\"),\n","        count(when(col(\"is_retained\") == True, True)).alias(\"retained_users\")\n","    ).collect()\n","\n","    # Verifica se temos ambos os grupos para comparar\n","    control_rows = [row for row in summary_df if row['is_target'] == 'control']\n","    target_rows = [row for row in summary_df if row['is_target'] == 'target']\n","\n","    if not control_rows or not target_rows:\n","        print(f\"AVISO: Dados insuficientes para o segmento '{segment_name}'. Análise pulada.\")\n","        continue\n","\n","    control_row = control_rows[0]\n","    target_row = target_rows[0]\n","\n","    control_total = control_row['total_users']\n","    control_retained = control_row['retained_users']\n","    target_total = target_row['total_users']\n","    target_retained = target_row['retained_users']\n","\n","    # Calcula as taxas de retenção\n","    retention_rate_control = control_retained / control_total if control_total > 0 else 0\n","    retention_rate_target = target_retained / target_total if target_total > 0 else 0\n","\n","    print(f\"Total de Usuários (Controle): {control_total}\")\n","    print(f\"Usuários Retidos (Controle): {control_retained}\")\n","    print(f\"Taxa de Retenção (Controle): {retention_rate_control:.2%}\")\n","    print(\"-\" * 30)\n","    print(f\"Total de Usuários (Teste): {target_total}\")\n","    print(f\"Usuários Retidos (Teste): {target_retained}\")\n","    print(f\"Taxa de Retenção (Teste): {retention_rate_target:.2%}\")\n","\n","    # Calcula o Lift\n","    lift = ((retention_rate_target - retention_rate_control) / retention_rate_control) * 100 if retention_rate_control > 0 else 0\n","    print(f\"\\nLift Percentual (Aumento): {lift:.2f}%\")\n","\n","    # Calcula o Intervalo de Confiança para a diferença das proporções\n","    diff = retention_rate_target - retention_rate_control\n","    if target_total > 0 and control_total > 0:\n","        std_dev1_sq = retention_rate_target * (1 - retention_rate_target) / target_total\n","        std_dev2_sq = retention_rate_control * (1 - retention_rate_control) / control_total\n","        std_err_diff = math.sqrt(std_dev1_sq + std_dev2_sq)\n","        conf_interval_diff = (diff - 1.96 * std_err_diff, diff + 1.96 * std_err_diff)\n","    else:\n","        conf_interval_diff = (0, 0)\n","\n","    print(f\"Diferença Absoluta: {diff:.2%}\")\n","    print(f\"Intervalo de Confiança 95% para a Diferença: [{conf_interval_diff[0]:.2%}, {conf_interval_diff[1]:.2%}]\")\n","\n","    # Armazena o resultado para a tabela final\n","    final_business_results_rfm.append({\n","        'Segmento': segment_name,\n","        'Retenção (Controle)': retention_rate_control,\n","        'Retenção (Teste)': retention_rate_target,\n","        'Lift (%)': lift\n","    })\n","\n","# --- Bloco Final: Apresentação da Tabela Resumo ---\n","print(\"\\n\\n\" + \"=\"*80)\n","print(\"### TABELA RESUMO: MÉTRICAS DE NEGÓCIO POR SEGMENTO RFM ###\")\n","print(\"=\"*80)\n","\n","df_final_biz_rfm = pd.DataFrame(final_business_results_rfm)\n","\n","# Formatando as colunas\n","df_final_biz_rfm['Retenção (Controle)'] = df_final_biz_rfm['Retenção (Controle)'].apply(lambda x: f'{x:.2%}')\n","df_final_biz_rfm['Retenção (Teste)'] = df_final_biz_rfm['Retenção (Teste)'].apply(lambda x: f'{x:.2%}')\n","df_final_biz_rfm['Lift (%)'] = df_final_biz_rfm['Lift (%)'].apply(lambda x: f'{x:.2f}%')\n","\n","# Ordenando por Lift para destacar os segmentos com maior impacto\n","print(df_final_biz_rfm.sort_values(by='Lift (%)', key=lambda x: x.str.replace('%', '').astype(float), ascending=False).to_string(index=False))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n_wHGGFWqp2Y","executionInfo":{"status":"ok","timestamp":1750533779874,"user_tz":180,"elapsed":488043,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"b2700a41-e8bb-40ec-cbff-d94f14cf870d"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","================================================================================\n","### ETAPA 2: ANÁLISE DE NEGÓCIO E IMPACTO POR SEGMENTO RFM ###\n","================================================================================\n","\n","------------------------------------------------------------\n","### Calculando Métricas de Negócio para o Segmento: Campeões ###\n","------------------------------------------------------------\n","Total de Usuários (Controle): 20917\n","Usuários Retidos (Controle): 0\n","Taxa de Retenção (Controle): 0.00%\n","------------------------------\n","Total de Usuários (Teste): 20803\n","Usuários Retidos (Teste): 0\n","Taxa de Retenção (Teste): 0.00%\n","\n","Lift Percentual (Aumento): 0.00%\n","Diferença Absoluta: 0.00%\n","Intervalo de Confiança 95% para a Diferença: [0.00%, 0.00%]\n","\n","------------------------------------------------------------\n","### Calculando Métricas de Negócio para o Segmento: Clientes Leais ###\n","------------------------------------------------------------\n","Total de Usuários (Controle): 50201\n","Usuários Retidos (Controle): 0\n","Taxa de Retenção (Controle): 0.00%\n","------------------------------\n","Total de Usuários (Teste): 50427\n","Usuários Retidos (Teste): 0\n","Taxa de Retenção (Teste): 0.00%\n","\n","Lift Percentual (Aumento): 0.00%\n","Diferença Absoluta: 0.00%\n","Intervalo de Confiança 95% para a Diferença: [0.00%, 0.00%]\n","\n","------------------------------------------------------------\n","### Calculando Métricas de Negócio para o Segmento: Em Risco / Hibernando ###\n","------------------------------------------------------------\n","Total de Usuários (Controle): 172390\n","Usuários Retidos (Controle): 117107\n","Taxa de Retenção (Controle): 67.93%\n","------------------------------\n","Total de Usuários (Teste): 229890\n","Usuários Retidos (Teste): 174668\n","Taxa de Retenção (Teste): 75.98%\n","\n","Lift Percentual (Aumento): 11.85%\n","Diferença Absoluta: 8.05%\n","Intervalo de Confiança 95% para a Diferença: [7.77%, 8.33%]\n","\n","------------------------------------------------------------\n","### Calculando Métricas de Negócio para o Segmento: Novos / Potenciais ###\n","------------------------------------------------------------\n","Total de Usuários (Controle): 48908\n","Usuários Retidos (Controle): 48908\n","Taxa de Retenção (Controle): 100.00%\n","------------------------------\n","Total de Usuários (Teste): 73804\n","Usuários Retidos (Teste): 73804\n","Taxa de Retenção (Teste): 100.00%\n","\n","Lift Percentual (Aumento): 0.00%\n","Diferença Absoluta: 0.00%\n","Intervalo de Confiança 95% para a Diferença: [0.00%, 0.00%]\n","\n","------------------------------------------------------------\n","### Calculando Métricas de Negócio para o Segmento: Outros ###\n","------------------------------------------------------------\n","Total de Usuários (Controle): 67283\n","Usuários Retidos (Controle): 5280\n","Taxa de Retenção (Controle): 7.85%\n","------------------------------\n","Total de Usuários (Teste): 69936\n","Usuários Retidos (Teste): 7792\n","Taxa de Retenção (Teste): 11.14%\n","\n","Lift Percentual (Aumento): 41.98%\n","Diferença Absoluta: 3.29%\n","Intervalo de Confiança 95% para a Diferença: [2.98%, 3.60%]\n","\n","\n","================================================================================\n","### TABELA RESUMO: MÉTRICAS DE NEGÓCIO POR SEGMENTO RFM ###\n","================================================================================\n","             Segmento Retenção (Controle) Retenção (Teste) Lift (%)\n","               Outros               7.85%           11.14%   41.98%\n","Em Risco / Hibernando              67.93%           75.98%   11.85%\n","             Campeões               0.00%            0.00%    0.00%\n","       Clientes Leais               0.00%            0.00%    0.00%\n","   Novos / Potenciais             100.00%          100.00%    0.00%\n"]}]},{"cell_type":"markdown","source":["## Análise das Métricas de Negócio e Conclusão: Impacto por Segmento RFM\n","\n","Com base na análise estatística e nos indicadores de negócio extraídos por segmentação RFM, é possível obter insights relevantes sobre o desempenho da campanha de retenção, considerando o comportamento passado dos usuários em relação à Recência, Frequência e Valor Monetário.\n","\n","---\n","\n","### Segmento: Em Risco / Hibernando\n","- **Taxa de Retenção (Controle):** 67.93%  \n","- **Taxa de Retenção (Teste):** 75.98%  \n","- **Lift Percentual:** +11.85%  \n","- **Diferença Absoluta:** +8.05 pontos percentuais  \n","- **Intervalo de Confiança 95%:** [7.77%, 8.33%]\n","\n","Este é um dos segmentos mais estratégicos da base: usuários que já tiveram histórico positivo, mas apresentam risco de abandono ou já se encontram inativos. A campanha mostrou excelente desempenho nesse grupo, conseguindo reengajar de forma significativa. Trata-se de um resultado acionável e diretamente aplicável para estratégias de winback e remarketing. Uma abordagem personalizada para esse grupo pode consolidar essa reversão de tendência.\n","\n","---\n","\n","### Segmento: Outros\n","- **Taxa de Retenção (Controle):** 7.85%  \n","- **Taxa de Retenção (Teste):** 11.14%  \n","- **Lift Percentual:** +41.98%  \n","- **Diferença Absoluta:** +3.29 pontos percentuais  \n","- **Intervalo de Confiança 95%:** [2.98%, 3.60%]\n","\n","Apesar da baixa taxa de retenção inicial, o grupo \"Outros\" apresentou o maior **lift relativo**. Esse segmento possivelmente concentra usuários com comportamentos não suficientemente consistentes para serem classificados em grupos tradicionais — ou seja, pode representar **clientes indecisos, novos em fase de experimentação ou com baixa frequência**. A campanha mostrou-se eficiente como uma possível “primeira âncora” para introduzir o app na rotina desses clientes. Estratégias de onboarding e nutrição automatizada podem transformar esses contatos em perfis mais fiéis ao longo do tempo.\n","\n","---\n","\n","### Segmentos com 0% e 100% de Retenção\n","\n","#### Campeões e Clientes Leais\n","- Ambos os grupos apresentaram **0% de retenção** para controle e teste.\n","- Isso indica possível inconsistência ou ausência de eventos de retenção registrados no período de análise para esses segmentos.\n","- Pode estar relacionado a filtros de tempo, definição de métrica de retenção ou mesmo a uma **base de dados incompleta** para usuários que, por padrão, não estavam expostos ao risco de churn.\n","\n","#### Novos / Potenciais\n","- Apresentaram **100% de retenção** tanto no grupo de controle quanto no de teste.\n","- Embora esse número pareça excelente, ele é irreal em contextos operacionais. Provavelmente estamos diante de:\n","  - Uma definição de retenção inadequada para o ciclo de vida curto desse grupo;\n","  - Ou uma limitação nos dados de observação no tempo (ex: usuários muito recentes).\n","- Recomendamos investigar a janela de tempo utilizada ou redefinir a métrica de retenção para este tipo de perfil.\n","\n","---\n","\n","## Conclusão Geral\n","\n","Os resultados apontam para uma eficácia comprovada da campanha de retenção em dois segmentos centrais:\n","\n","1. **\"Em Risco / Hibernando\"** — usuários com forte histórico que estavam se distanciando. A campanha teve efeito expressivo de reativação.\n","2. **\"Outros\"** — clientes indefinidos, com resposta positiva ao estímulo, indicando potencial para nutrição e fidelização.\n","\n","Já os segmentos com 0% ou 100% de retenção devem ser tratados com cautela. São importantes para entender limitações dos dados e das definições de métrica de sucesso, podendo indicar necessidade de ajustes na modelagem ou observação de tempo.\n","\n","Assim, as evidências reforçam que a segmentação RFM é um critério eficaz para ações de retenção, com resultados acionáveis em perfis estratégicos e oportunidades claras de otimização.\n"],"metadata":{"id":"PXIOP7dYwePF"}},{"cell_type":"markdown","source":["## Introdução: Análise Final de Impacto por Segmentos Combinados (Consumo + RFM)\n","\n","Com base nas análises estatísticas e de negócio realizadas separadamente por **padrão de consumo** e por **perfil RFM**, foi possível identificar dois pontos de atenção estratégicos:\n","\n","1. **Usuários \"Em Risco / Hibernando\"** apresentaram forte resposta à campanha, com melhoria estatisticamente significativa na taxa de retenção, validando o potencial de ações de reengajamento para esse público.\n","2. **Usuários classificados como \"Outros\"** — embora com baixa retenção inicial — mostraram um dos maiores *lifts percentuais* após a campanha. Esse grupo pode representar clientes em fase de descoberta ou uso esporádico, e a campanha parece ter funcionado como um ponto de entrada eficaz.\n","\n","A partir desses achados, esta etapa propõe uma **análise combinada entre os dois eixos de segmentação**: comportamento de consumo e estágio de relacionamento (RFM). O objetivo é identificar interseções que potencializem a ação de retenção, ampliando o entendimento sobre **quais perfis compostos respondem melhor à campanha**.\n","\n","Serão avaliadas as seguintes combinações:\n","- **Usuários de Fim de Semana + Em Risco / Hibernando**\n","- **Usuários de Dias de Semana + Em Risco / Hibernando**\n","- **Usuários de Fim de Semana + Outros**\n","- **Usuários de Dias de Semana + Outros**\n","\n","A análise considera tanto a **significância estatística** (via teste Qui-Quadrado), quanto **indicadores de negócio** como taxa de retenção, lift percentual e intervalo de confiança da diferença absoluta.\n","\n","Este aprofundamento visa apoiar decisões de personalização e segmentação mais refinadas, oferecendo **insights acionáveis para estratégias de retenção direcionadas a perfis compostos específicos**.\n"],"metadata":{"id":"XgzT5P2nw78M"}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer, VectorAssembler\n","from pyspark.ml.stat import ChiSquareTest\n","from pyspark.sql.functions import *\n","\n","import math\n","\n","print(\"\\n\\n\" + \"=\"*100)\n","print(\"### ANÁLISE FINAL: SEGMENTOS COMBINADOS (PADRÃO DE CONSUMO + RFM) ###\")\n","print(\"=\"*100)\n","\n","# Mapeamentos legíveis\n","map_consumo = {'1': 'Usuário de Fim de Semana', '2': 'Usuário de Dias de Semana'}\n","map_rfm = {'3': 'Em Risco / Hibernando','5': 'Outros'}\n","\n","# Unimos as segmentações para criar um DataFrame combinado\n","dfp_full = dfp_analysis \\\n","    .join(dfp_final_segmentation_pattern, \"customer_id\", \"inner\") \\\n","    .join(dfp_final_segmentation_rfm, \"customer_id\", \"inner\")\n","\n","# Lista para guardar os resultados\n","final_cross_results = []\n","\n","# Itera sobre as combinações desejadas\n","for consumo_code in ['1', '2']:  # Fim de Semana e Dias de Semana\n","    for rfm_code in ['3','5']:       #  \"Em Risco\" e Outros\n","        consumo_label = map_consumo.get(consumo_code)\n","        rfm_label = map_rfm.get(rfm_code)\n","        segment_label = f\"{consumo_label} + {rfm_label}\"\n","\n","        print(\"\\n\" + \"-\"*60)\n","        print(f\"### Segmento Combinado: {segment_label} ###\")\n","        print(\"-\"*60)\n","\n","        # Filtra o segmento desejado\n","        df_segment = dfp_full.filter(\n","            (col(\"consumption_segment\") == consumo_code) &\n","            (col(\"rfm_segment\") == rfm_code)\n","        )\n","\n","        if df_segment.count() < 100:  # Pode ajustar esse número\n","            print(\"⚠️  Amostra insuficiente para análise robusta. Pulando...\")\n","            continue\n","\n","        # Conversões para teste estatístico\n","        df_segment = df_segment.withColumn(\"is_retained_int\", col(\"is_retained\").cast(\"integer\"))\n","        df_segment = StringIndexer(inputCol=\"is_target\", outputCol=\"label\").fit(df_segment).transform(df_segment)\n","        df_segment = VectorAssembler(inputCols=[\"is_retained_int\"], outputCol=\"features\").transform(df_segment)\n","\n","        # Teste Qui-Quadrado\n","        chi_square_result = ChiSquareTest.test(df_segment, \"features\", \"label\").first()\n","        p_value = float(chi_square_result.asDict()[\"pValues\"][0])\n","        chi2_statistic = float(chi_square_result.asDict()[\"statistics\"][0])\n","\n","        print(f\"P-Valor: {p_value:.8f}\")\n","        print(f\"Estatística Qui-Quadrado: {chi2_statistic:.4f}\")\n","        if p_value < 0.05:\n","            print(\"✅ Diferença estatisticamente significativa.\")\n","        else:\n","            print(\"❌ Diferença NÃO estatisticamente significativa.\")\n","\n","        # Métricas de negócio\n","        summary_df = df_segment.groupBy(\"is_target\").agg(\n","            count(\"*\").alias(\"total_users\"),\n","            count(when(col(\"is_retained\") == True, True)).alias(\"retained_users\")\n","        ).collect()\n","\n","        control = [r for r in summary_df if r[\"is_target\"] == \"control\"][0]\n","        target = [r for r in summary_df if r[\"is_target\"] == \"target\"][0]\n","\n","        control_total = control[\"total_users\"]\n","        control_retained = control[\"retained_users\"]\n","        target_total = target[\"total_users\"]\n","        target_retained = target[\"retained_users\"]\n","\n","        rate_control = control_retained / control_total\n","        rate_target = target_retained / target_total\n","        lift = ((rate_target - rate_control) / rate_control) * 100 if rate_control > 0 else 0\n","        diff = rate_target - rate_control\n","\n","        std_dev1_sq = rate_target * (1 - rate_target) / target_total\n","        std_dev2_sq = rate_control * (1 - rate_control) / control_total\n","        std_err_diff = math.sqrt(std_dev1_sq + std_dev2_sq)\n","        conf_interval_diff = (diff - 1.96 * std_err_diff, diff + 1.96 * std_err_diff)\n","\n","        print(f\"\\n--- Métricas de Negócio ---\")\n","        print(f\"Taxa de Retenção (Controle): {rate_control:.2%}\")\n","        print(f\"Taxa de Retenção (Teste): {rate_target:.2%}\")\n","        print(f\"Lift Percentual: {lift:.2f}%\")\n","        print(f\"Diferença Absoluta: {diff:.2%}\")\n","        print(f\"Intervalo de Confiança 95%: [{conf_interval_diff[0]:.2%}, {conf_interval_diff[1]:.2%}]\")\n","\n","        # Armazenamento\n","        final_cross_results.append({\n","            \"Segmento Combinado\": segment_label,\n","            \"Retenção (Controle)\": f\"{rate_control:.2%}\",\n","            \"Retenção (Teste)\": f\"{rate_target:.2%}\",\n","            \"Lift (%)\": f\"{lift:.2f}%\",\n","            \"p-value\": p_value,\n","            \"IC 95%\": f\"[{conf_interval_diff[0]:.2%}, {conf_interval_diff[1]:.2%}]\"\n","        })\n","\n","# Mostrar resultados finais como tabela\n","df_result = pd.DataFrame(final_cross_results)\n","print(\"\\n\\n\" + \"=\"*80)\n","print(\"### TABELA FINAL: Análise de Impacto por Segmento Combinado ###\")\n","print(\"=\"*80)\n","print(df_result.to_string(index=False))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2JhcfIdNV0dw","executionInfo":{"status":"ok","timestamp":1750535250979,"user_tz":180,"elapsed":1471107,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"65eabd7a-347b-4f0a-a4be-a68155dfcf48"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","====================================================================================================\n","### ANÁLISE FINAL: SEGMENTOS COMBINADOS (PADRÃO DE CONSUMO + RFM) ###\n","====================================================================================================\n","\n","------------------------------------------------------------\n","### Segmento Combinado: Usuário de Fim de Semana + Em Risco / Hibernando ###\n","------------------------------------------------------------\n","P-Valor: 0.00000000\n","Estatística Qui-Quadrado: 615.0874\n","✅ Diferença estatisticamente significativa.\n","\n","--- Métricas de Negócio ---\n","Taxa de Retenção (Controle): 38.65%\n","Taxa de Retenção (Teste): 49.01%\n","Lift Percentual: 26.80%\n","Diferença Absoluta: 10.36%\n","Intervalo de Confiança 95%: [9.54%, 11.17%]\n","\n","------------------------------------------------------------\n","### Segmento Combinado: Usuário de Fim de Semana + Outros ###\n","------------------------------------------------------------\n","P-Valor: 0.00000000\n","Estatística Qui-Quadrado: 137.0084\n","✅ Diferença estatisticamente significativa.\n","\n","--- Métricas de Negócio ---\n","Taxa de Retenção (Controle): 4.49%\n","Taxa de Retenção (Teste): 6.65%\n","Lift Percentual: 47.97%\n","Diferença Absoluta: 2.16%\n","Intervalo de Confiança 95%: [1.80%, 2.52%]\n","\n","------------------------------------------------------------\n","### Segmento Combinado: Usuário de Dias de Semana + Em Risco / Hibernando ###\n","------------------------------------------------------------\n","P-Valor: 0.00000000\n","Estatística Qui-Quadrado: 1654.5041\n","✅ Diferença estatisticamente significativa.\n","\n","--- Métricas de Negócio ---\n","Taxa de Retenção (Controle): 50.20%\n","Taxa de Retenção (Teste): 59.86%\n","Lift Percentual: 19.25%\n","Diferença Absoluta: 9.66%\n","Intervalo de Confiança 95%: [9.20%, 10.13%]\n","\n","------------------------------------------------------------\n","### Segmento Combinado: Usuário de Dias de Semana + Outros ###\n","------------------------------------------------------------\n","P-Valor: 0.00000000\n","Estatística Qui-Quadrado: 144.3506\n","✅ Diferença estatisticamente significativa.\n","\n","--- Métricas de Negócio ---\n","Taxa de Retenção (Controle): 4.72%\n","Taxa de Retenção (Teste): 6.85%\n","Lift Percentual: 45.16%\n","Diferença Absoluta: 2.13%\n","Intervalo de Confiança 95%: [1.78%, 2.48%]\n","\n","\n","================================================================================\n","### TABELA FINAL: Análise de Impacto por Segmento Combinado ###\n","================================================================================\n","                               Segmento Combinado Retenção (Controle) Retenção (Teste) Lift (%)  p-value          IC 95%\n"," Usuário de Fim de Semana + Em Risco / Hibernando              38.65%           49.01%   26.80%      0.0 [9.54%, 11.17%]\n","                Usuário de Fim de Semana + Outros               4.49%            6.65%   47.97%      0.0  [1.80%, 2.52%]\n","Usuário de Dias de Semana + Em Risco / Hibernando              50.20%           59.86%   19.25%      0.0 [9.20%, 10.13%]\n","               Usuário de Dias de Semana + Outros               4.72%            6.85%   45.16%      0.0  [1.78%, 2.48%]\n"]}]},{"cell_type":"code","source":["df_filtered_full = dfp_full.filter(((col(\"consumption_segment\") == '1') | (col(\"consumption_segment\") == '2')) & ((col(\"rfm_segment\") == '3') | (col(\"rfm_segment\") == '5')))\n","\n","\n","dfp_total_customers_per_group = df_filtered_full.groupBy(\"is_target\",\"is_retained\",\"consumption_segment\",\"rfm_segment\").agg(\n","    count(\"customer_id\").alias(\"total_customers\")\n",")\n","dfp_total_customers_per_group.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g1n-j3QZ1fXC","executionInfo":{"status":"ok","timestamp":1750535356538,"user_tz":180,"elapsed":105570,"user":{"displayName":"Lucas Brito","userId":"07232248405911347836"}},"outputId":"322450cb-fcf2-4aa6-deee-29a9d9133ffa"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+-----------+-------------------+-----------+---------------+\n","|is_target|is_retained|consumption_segment|rfm_segment|total_customers|\n","+---------+-----------+-------------------+-----------+---------------+\n","|   target|       true|                  2|          3|          58920|\n","|  control|      false|                  2|          3|          39238|\n","|   target|      false|                  1|          5|          29351|\n","|  control|      false|                  1|          5|          29347|\n","|   target|       true|                  1|          3|          15098|\n","|  control|       true|                  1|          3|          10108|\n","|  control|      false|                  1|          3|          16045|\n","|   target|      false|                  2|          3|          39512|\n","|  control|      false|                  2|          5|          32656|\n","|   target|      false|                  2|          5|          32793|\n","|  control|       true|                  2|          3|          39548|\n","|   target|      false|                  1|          3|          15710|\n","|   target|       true|                  1|          5|           2091|\n","|  control|       true|                  2|          5|           1617|\n","|   target|       true|                  2|          5|           2411|\n","|  control|       true|                  1|          5|           1381|\n","+---------+-----------+-------------------+-----------+---------------+\n","\n"]}]},{"cell_type":"markdown","source":["## Conclusão: Impacto da Campanha em Segmentos Combinados (Padrão de Consumo + RFM)\n","\n","Com base na análise estatística e nos indicadores de negócio gerados a partir da combinação entre os segmentos de **padrão de consumo** e **perfil RFM**, foi possível identificar interseções que se destacam tanto pelo impacto da campanha quanto pelo potencial de ação estratégica.\n","\n","### Segmento: Usuário de Fim de Semana + Em Risco / Hibernando\n","- **Taxa de Retenção (Controle):** 38.65%\n","- **Taxa de Retenção (Teste):** 49.01%\n","- **Lift Percentual:** +26.80%\n","- **Diferença Absoluta:** +10.36 pontos percentuais\n","- **Intervalo de Confiança 95%:** [9.54%, 11.17%]\n","\n","Este grupo une dois perfis já validados individualmente como responsivos: usuários em risco e com forte comportamento de uso concentrado nos fins de semana. A campanha demonstrou um impacto substancial e estatisticamente significativo, indicando alto potencial de retorno com estratégias específicas de reengajamento nesse perfil. Abordagens com gatilhos personalizados em dias estratégicos podem amplificar ainda mais os resultados.\n","\n","### Segmento: Usuário de Dias de Semana + Em Risco / Hibernando\n","- **Taxa de Retenção (Controle):** 50.20%\n","- **Taxa de Retenção (Teste):** 59.86%\n","- **Lift Percentual:** +19.25%\n","- **Diferença Absoluta:** +9.66 pontos percentuais\n","- **Intervalo de Confiança 95%:** [9.20%, 10.13%]\n","\n","Assim como seu correspondente de fim de semana, este segmento confirmou o bom desempenho da campanha entre perfis com risco de churn, especialmente os que interagem nos dias úteis. Campanhas de reativação integradas à rotina semanal — como notificações durante a semana ou benefícios em dias úteis — podem ser chave para manter esse público engajado.\n","\n","### Segmento: Usuário de Fim de Semana + Outros\n","- **Taxa de Retenção (Controle):** 4.49%\n","- **Taxa de Retenção (Teste):** 6.65%\n","- **Lift Percentual:** +47.97%\n","- **Diferença Absoluta:** +2.16 pontos percentuais\n","- **Intervalo de Confiança 95%:** [1.80%, 2.52%]\n","\n","Apesar da base relativamente pequena, o lift percentual foi expressivo. O grupo “Outros” pode refletir perfis menos consistentes ou em fase de descoberta do app. A campanha pode ter atuado como ponto de entrada ou reforço inicial para consolidar o uso. Estratégias como onboarding otimizado, incentivos progressivos ou tutoriais gamificados podem ser especialmente eficazes nesse público.\n","\n","### Segmento: Usuário de Dias de Semana + Outros\n","- **Taxa de Retenção (Controle):** 4.72%\n","- **Taxa de Retenção (Teste):** 6.85%\n","- **Lift Percentual:** +45.16%\n","- **Diferença Absoluta:** +2.13 pontos percentuais\n","- **Intervalo de Confiança 95%:** [1.78%, 2.48%]\n","\n","Perfil semelhante ao anterior, mas com comportamento de uso durante os dias úteis. A boa resposta à campanha reforça a hipótese de que este grupo reúne usuários em fase inicial ou com uso esporádico. Estratégias leves e recorrentes de ativação podem funcionar como mecanismos de transição para perfis mais fiéis.\n","\n","---\n","\n","## Conclusão Geral\n","\n","A análise combinada mostra que a intersecção entre padrões de uso e perfis de relacionamento pode revelar **segmentos de alto valor estratégico**. Dois grandes grupos se destacam:\n","\n","- **Em Risco / Hibernando + qualquer padrão de consumo:** Resposta sólida à campanha, com impacto robusto e replicável. Estratégias de reengajamento personalizadas são recomendadas.\n","- **Segmento “Outros”:** Apesar das baixas taxas iniciais, esse grupo mostrou o maior *lift percentual*, sinalizando que a campanha pode estar atuando como uma porta de entrada para consolidar o uso do app. Investir nesse perfil pode representar **uma oportunidade de conversão de clientes indecisos em usuários ativos**.\n","\n","Com esses achados, torna-se evidente que **ações personalizadas por perfis compostos podem aumentar substancialmente a eficácia de campanhas de retenção**, orientando investimentos e comunicações de forma mais precisa e orientada por dados.\n"],"metadata":{"id":"hwfMExdTeZog"}},{"cell_type":"code","source":["def sink_data_gold(df,gold_path):\n","  df.write.mode(\"overwrite\").parquet(gold_path)\n"],"metadata":{"id":"6Z1z84kQlZ5m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drive_base_path = \"drive/MyDrive/iFood/Data/\"\n","\n","if data_source == \"drive\":\n","    # Define o caminho para a camada 'Gold' dentro da estrutura do Drive.\n","    dfp_orders_filtered_gold = f\"{drive_base_path}/Gold/dfp_orders_filtered.parquet\"\n","    dfp_analysis_gold = f\"{drive_base_path}/Gold/dfp_analysis.parquet\"\n","    dfp_master_consumption_gold = f\"{drive_base_path}/Gold/dfp_master_consumption.parquet\"\n","    dfp_master_rfm_gold = f\"{drive_base_path}/Gold/dfp_master_rfm.parquet\"\n","    dfp_final_segmentation_rfm_gold = f\"{drive_base_path}/Gold/dfp_final_segmentation_rfm.parquet\"\n","    dfp_final_segmentation_pattern_gold = f\"{drive_base_path}/Gold/dfp_final_segmentation_pattern.parquet\"\n","    df_result_gold_path = f\"{drive_base_path}/Gold/df_result_consumption.parquet\"\n","    df_final_biz_gold_path = f\"{drive_base_path}/Gold/df_result_rfm.parquet\"\n","    df_final_biz_rfm_gold_path = f\"{drive_base_path}/Gold/df_result_rfm_final.parquet\"\n","\n","\n","\n","sink_data_gold(dfp_orders_filtered, dfp_orders_filtered_gold)\n","sink_data_gold(dfp_analysis, dfp_analysis_gold)\n","sink_data_gold(dfp_master_consumption, dfp_master_consumption_gold)\n","sink_data_gold(dfp_master_rfm, dfp_master_rfm_gold)\n","sink_data_gold(dfp_final_segmentation_rfm, dfp_final_segmentation_rfm_gold)\n","sink_data_gold(dfp_final_segmentation_pattern, dfp_final_segmentation_pattern_gold)\n","df_result.to_parquet(df_result_gold_path)\n","df_final_biz.to_parquet(df_final_biz_gold_path)\n","df_final_biz_rfm.to_parquet(df_final_biz_rfm_gold_path)\n","\n"],"metadata":{"id":"P8aTLpEtlhjT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n"],"metadata":{"id":"XsqV4MxSnPFQ"},"execution_count":null,"outputs":[]}]}